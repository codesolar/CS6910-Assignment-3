{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\ninstalling wandb and indic package\n'''\n!pip install wandb\n!pip install indic-nlp-library","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-21T09:35:39.680627Z","iopub.execute_input":"2023-05-21T09:35:39.681043Z","iopub.status.idle":"2023-05-21T09:36:03.802668Z","shell.execute_reply.started":"2023-05-21T09:35:39.680991Z","shell.execute_reply":"2023-05-21T09:36:03.801437Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.28.2)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.4)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.20.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: indic-nlp-library in /opt/conda/lib/python3.10/site-packages (0.92)\nRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.2.4)\nRequirement already satisfied: sphinx-argparse in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.23.5)\nRequirement already satisfied: morfessor in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (2.0.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.5.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.3)\nRequirement already satisfied: sphinx>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx-argparse->indic-nlp-library) (7.0.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->indic-nlp-library) (1.16.0)\nRequirement already satisfied: sphinxcontrib-devhelp in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\nRequirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.19)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.12.1)\nRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\nRequirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\nRequirement already satisfied: sphinxcontrib-jsmath in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\nRequirement already satisfied: imagesize>=1.3 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\nRequirement already satisfied: sphinxcontrib-applehelp in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.4)\nRequirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\nRequirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.28.2)\nRequirement already satisfied: sphinxcontrib-qthelp in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.3)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\nRequirement already satisfied: Pygments>=2.13 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.15.0)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.12.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"'''\nall important libraries are called\n'''\nfrom indicnlp.langinfo import *\nlang = 'hi'\nimport wandb\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist, fashion_mnist\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport shutil\nimport torch.nn.functional as F\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:11.998323Z","iopub.execute_input":"2023-05-21T09:36:11.999221Z","iopub.status.idle":"2023-05-21T09:36:12.008433Z","shell.execute_reply.started":"2023-05-21T09:36:11.999182Z","shell.execute_reply":"2023-05-21T09:36:12.007412Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"'''\ndevice variable is set\n'''\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:13.794252Z","iopub.execute_input":"2023-05-21T09:36:13.794664Z","iopub.status.idle":"2023-05-21T09:36:13.801873Z","shell.execute_reply.started":"2023-05-21T09:36:13.794634Z","shell.execute_reply":"2023-05-21T09:36:13.800618Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"'''\ntrain path , valid path , test path are written\n'''\ntrain_path = \"/kaggle/input/dataset/aksharantar_sampled/hin/hin_train.csv\"\nvalid_path = \"/kaggle/input/dataset/aksharantar_sampled/hin/hin_valid.csv\"\ntest_path = \"/kaggle/input/dataset/aksharantar_sampled/hin/hin_test.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:15.233982Z","iopub.execute_input":"2023-05-21T09:36:15.234578Z","iopub.status.idle":"2023-05-21T09:36:15.241638Z","shell.execute_reply.started":"2023-05-21T09:36:15.234539Z","shell.execute_reply":"2023-05-21T09:36:15.240505Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"'''\nthe train test validation set are these\n'''\ndf_train = pd.read_csv(train_path,names = ['X','y'])\ndf_valid = pd.read_csv(valid_path,names = ['X','y'])\ndf_test = pd.read_csv(test_path,names = ['X','y'])","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:16.499445Z","iopub.execute_input":"2023-05-21T09:36:16.500159Z","iopub.status.idle":"2023-05-21T09:36:16.621350Z","shell.execute_reply.started":"2023-05-21T09:36:16.500126Z","shell.execute_reply":"2023-05-21T09:36:16.620472Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"'''\ntarget word's maxlength(Hindi) and source word's maxlength(english) are these\n'''\ntargetmaxlen = 0\nsourcemaxlen = 0\nfor word in df_train.iloc[:,1]:\n    targetmaxlen = max(targetmaxlen,len(word))\nfor word in df_train.iloc[:,0]:\n    sourcemaxlen = max(sourcemaxlen,len(word))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:17.787130Z","iopub.execute_input":"2023-05-21T09:36:17.787955Z","iopub.status.idle":"2023-05-21T09:36:17.852344Z","shell.execute_reply.started":"2023-05-21T09:36:17.787911Z","shell.execute_reply":"2023-05-21T09:36:17.851495Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"'''\nstart of the word and end of the word are declared\n'''\nSOW_token = 0\nEOW_token = 1\n\n'''\nLang class is created and two instances of it are created which are for source and target language\n'''\nclass Lang:\n    def __init__(self):\n        self.char2index = {}\n        self.index2char = {0: '0', 1: '1'} #'0' sow and '1' eow\n        self.n_chars = 2  # Count SOS and EOS\n\n\n    def addWord(self, word):\n        for c in word:\n            if c not in self.char2index:\n                self.char2index[c] = self.n_chars\n                self.index2char[self.n_chars] = c\n                self.n_chars += 1","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:19.077705Z","iopub.execute_input":"2023-05-21T09:36:19.078400Z","iopub.status.idle":"2023-05-21T09:36:19.085411Z","shell.execute_reply.started":"2023-05-21T09:36:19.078333Z","shell.execute_reply":"2023-05-21T09:36:19.084424Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"lang_input = Lang()\nlang_output = Lang()\n# li = list(df_train['X'])","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:20.210265Z","iopub.execute_input":"2023-05-21T09:36:20.211267Z","iopub.status.idle":"2023-05-21T09:36:20.216547Z","shell.execute_reply.started":"2023-05-21T09:36:20.211225Z","shell.execute_reply":"2023-05-21T09:36:20.215458Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"'''\nAll the unique character present in both source and target words are stored and given unique number \nTrain Test Valid set all are used to know the unique characters of the words \nOtherwise good mapping of characters to number would not be possible\n'''\n\nenglish_words_train = list(df_train['X'])\nfor word in english_words_train:\n    lang_input.addWord(word)\nhindi_words_train = list(df_train['y'])\nfor word in hindi_words_train:\n    lang_output.addWord(word)\n    \n    \nenglish_words_valid = list(df_valid['X'])\nfor word in english_words_valid:\n    lang_input.addWord(word)\nhindi_words_valid = list(df_valid['y'])\nfor word in hindi_words_valid:\n    lang_output.addWord(word)\n    \n    \nenglish_words_test = list(df_test['X'])\nfor word in english_words_test:\n    lang_input.addWord(word)\nhindi_words_test = list(df_test['y'])\nfor word in hindi_words_test:\n    lang_output.addWord(word)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:21.365635Z","iopub.execute_input":"2023-05-21T09:36:21.366142Z","iopub.status.idle":"2023-05-21T09:36:21.530954Z","shell.execute_reply.started":"2023-05-21T09:36:21.366097Z","shell.execute_reply":"2023-05-21T09:36:21.530040Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"def indexesFromWord(lang, word):\n    li = []\n    for c in word:\n        li.append(lang.char2index[c])\n    return li\n\n\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:23.496565Z","iopub.execute_input":"2023-05-21T09:36:23.496968Z","iopub.status.idle":"2023-05-21T09:36:23.505204Z","shell.execute_reply.started":"2023-05-21T09:36:23.496936Z","shell.execute_reply":"2023-05-21T09:36:23.503989Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"input_batch_big = []\noutput_batch_big = []\ninput_tensor = [tensorFromWord(lang_input,word) for word in english_words_train]\noutput_tensor = [tensorFromWord(lang_output,word) for word in hindi_words_train]\ninput_batch_big.append(nn.utils.rnn.pad_sequence(input_tensor[0:len(english_words_train)]).squeeze(2).to(device)) #maxlength,batchsize,1\noutput_batch_big.append(nn.utils.rnn.pad_sequence(output_tensor[0:len(english_words_train)]).squeeze(2).to(device)) #maxlength,batchsize,1\nimaxlength = input_batch_big[0].size()[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:24.827164Z","iopub.execute_input":"2023-05-21T09:36:24.827577Z","iopub.status.idle":"2023-05-21T09:36:30.103248Z","shell.execute_reply.started":"2023-05-21T09:36:24.827544Z","shell.execute_reply":"2023-05-21T09:36:30.102296Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# input_batch_big2 = []\n# output_batch_big2 = []\n# input_tensor = [tensorFromWord(lang_input,word) for word in english_words_valid]\n# output_tensor = [tensorFromWord(lang_output,word) for word in hindi_words_valid]\n# input_batch_big.append(nn.utils.rnn.pad_sequence(input_tensor[0:len(english_words_valid)]).squeeze(2).to(device)) #maxlength,batchsize,1\n# output_batch_big.append(nn.utils.rnn.pad_sequence(output_tensor[0:len(english_words_valid)]).squeeze(2).to(device)) #maxlength,batchsize,1\n# imaxlength2 = input_batch_big2[0].size()[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:31.648061Z","iopub.execute_input":"2023-05-21T09:36:31.648431Z","iopub.status.idle":"2023-05-21T09:36:31.654091Z","shell.execute_reply.started":"2023-05-21T09:36:31.648401Z","shell.execute_reply":"2023-05-21T09:36:31.653180Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"'''\nEncoder class is here .\nIt extends nn of pytorch \ninput_size = Vocabulary size of source language(integer)\nhidden_size = number of neurons in the hidden layer of the encoder(integer)\ndrop_out = probability of a node being dropped out(number between 0 to 1)\nnum_layers = How many layers does the encoder has , These layers are stacked, one above another(Integer)\nbatchsize = total number of word pair in a batch(Integer)\nembeddingsize = The input (source word) characters are each converted to some embedding \n                by passing through one embedding layer , this denotes the size of that \n                embedding layer(Integer)\nbidirectional = if the encoder is bidirectional or not (takes boolean True / False)\nmodelname = name of the model (can be RNN , GRU , LSTM )(str) \ninput = Batch of words or a single word\nencoder_hidden = a tuple containing ( hidden , cell ) , LSTM has cell , so I kept a cell \n                It is used when needed , hidden and cell are initialised with initHidden method\nreturns output , a tuple containing hidden and cell state\n'''\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size,drop_out,num_layers,batchsize,embeddingsize,bidirectional,modelname):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.batchsize = batchsize\n        self.drop_out = drop_out\n        self.num_layers = num_layers\n        self.embeddingsize = embeddingsize\n        self.embedding = nn.Embedding(input_size, embeddingsize)\n        self.bidirectional = bidirectional\n        self.D = 1\n        if self.bidirectional == True:\n            self.D = 2\n        self.modelname = modelname\n        if modelname == 'RNN':\n            self.model = nn.RNN(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n        elif modelname == 'GRU':\n            self.model = nn.GRU(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n        elif modelname == 'LSTM':\n            self.model = nn.LSTM(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n\n        self.dropout = nn.Dropout(self.drop_out)\n\n    def forward(self, input, encoder_hidden):\n        embedded = self.embedding(input).view(-1,self.batchsize,self.embeddingsize)\n        maxlength = embedded.size()[0]\n        (hidden ,cell) = encoder_hidden\n        output = self.dropout(embedded)\n\n    \n        if self.modelname == 'RNN':\n            output, hidden = self.model(output, hidden)\n            cell = None\n        elif self.modelname == 'GRU':\n            output, hidden = self.model(output, hidden)\n            cell = None\n        elif self.modelname == 'LSTM':\n            output, (hidden,cell) = self.model(output, (hidden,cell))\n\n        if self.D == 2:\n            h = 0\n            o = 0\n            newoutput = torch.zeros(maxlength, self.batchsize, self.hidden_size, device = device)\n            newhidden = torch.zeros(self.num_layers, self.batchsize, self.hidden_size, device = device)\n            for i in range(0,self.D * self.num_layers,2):\n                newhidden[h, : , :] = torch.div(hidden[i, :, :].add(hidden[i + 1, :, :]),2)\n                h += 1\n            hidden = newhidden\n            for i in range(0,self.D * self.hidden_size,2):\n                newoutput[: , : ,o] = torch.div(output[: ,: ,i].add(output[: , : ,i + 1]),2)\n                o += 1\n            output = newoutput\n            if cell != None:\n                c = 0\n                newcell = torch.zeros(self.num_layers, self.batchsize, self.hidden_size, device = device)\n                for i in range(0,self.D * self.num_layers,2):\n                    newcell[c, : , :] = torch.div(cell[i, :, :].add(cell[i + 1, :, :]),2)\n                    c += 1\n                cell = newcell\n        return output, (hidden,cell)\n\n    def initHidden(self):\n        return torch.zeros(self.D * self.num_layers, self.batchsize, self.hidden_size, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:32.938908Z","iopub.execute_input":"2023-05-21T09:36:32.939281Z","iopub.status.idle":"2023-05-21T09:36:32.962628Z","shell.execute_reply.started":"2023-05-21T09:36:32.939249Z","shell.execute_reply":"2023-05-21T09:36:32.961430Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"'''\nDecoder class is here .\nIt extends nn of pytorch \nhidden_size = number of neurons in the hidden layer of the encoder(integer)\noutput_size = Vocabulary size of target language(integer)\ndrop_out = probability of a node being dropped out(number between 0 to 1)\nnum_layers = How many layers does the decoder has , These layers are stacked, one above another(Integer)\nbatchsize = total number of word pair in a batch(Integer)\nembeddingsize = The word (target word) characters are each converted to some embedding \n                by passing through one embedding layer , this denotes the size of that \n                embedding layer(Integer)\nmodelname = name of the model (can be RNN , GRU , LSTM )(str) \ninput = Batch of words or a single word or batch of words at a timestamp \nencoder_hidden = a tuple containing ( hidden , cell ) , LSTM has cell , so I kept a cell \n                It is used when needed , hidden and cell are initialised with initHidden method\nencoder_output = output of encoder (needed in attention implementation)\n\n\nreturns output , a tuple of hidden and cell state , attention weights (None as it is a normal decoder)\n'''\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size,drop_out,num_layers,batchsize,embeddingsize,modelname):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.batchsize = batchsize\n        self.num_layers = num_layers\n        self.drop_out = drop_out\n        self.embeddingsize = embeddingsize\n        self.embedding = nn.Embedding(output_size, embeddingsize)\n        self.modelname = modelname\n        if modelname == 'RNN':\n            self.model = nn.RNN(embeddingsize, hidden_size,num_layers)\n        elif modelname == 'GRU':\n            self.model = nn.GRU(embeddingsize, hidden_size,num_layers)\n        elif modelname == 'LSTM':\n            self.model = nn.LSTM(embeddingsize, hidden_size,num_layers)\n\n        self.dropout = nn.Dropout(self.drop_out)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, encoder_hidden,encoder_output):\n        output = self.embedding(input).view(-1, self.batchsize, self.embeddingsize)\n        output = self.dropout(output)\n        output = F.relu(output)\n        (hidden, cell) = encoder_hidden\n        if self.modelname == 'RNN':\n            output, hidden = self.model(output, hidden)\n        elif self.modelname == 'GRU':\n            output, hidden = self.model(output, hidden)\n        elif self.modelname == 'LSTM':\n            output, (hidden,cell) = self.model(output, (hidden,cell))\n        \n        output = self.softmax(self.out(output[0]))#dim = [128,67] before applying softmax\n        return output, (hidden,cell),None","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:34.677922Z","iopub.execute_input":"2023-05-21T09:36:34.678304Z","iopub.status.idle":"2023-05-21T09:36:34.691932Z","shell.execute_reply.started":"2023-05-21T09:36:34.678275Z","shell.execute_reply":"2023-05-21T09:36:34.690938Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"# '''\n# AttnDecoder class is here .\n# It extends nn of pytorch \n# hidden_size = number of neurons in the hidden layer of the encoder(integer)\n# output_size = Vocabulary size of target language(integer)\n# drop_out = probability of a node being dropped out(number between 0 to 1)\n# num_layers = How many layers does the decoder has , These layers are stacked, one above another(Integer)\n# batchsize = total number of word pair in a batch(Integer)\n# embeddingsize = The word (target word) characters are each converted to some embedding \n#                 by passing through one embedding layer , this denotes the size of that \n#                 embedding layer(Integer)\n# modelname = name of the model (can be RNN , GRU , LSTM )(str) \n# input = Batch of words or a single word or batch of words at a timestamp \n# encoder_hidden = a tuple containing ( hidden , cell ) , LSTM has cell , so I kept a cell \n#                 It is used when needed , hidden and cell are initialised with initHidden method\n# encoder_output = output of encoder (needed in attention implementation)\n\n\n# returns output , a tuple of hidden and cell state , attention weights (None as it is a normal decoder)\n# '''\n\n# class AttnDecoderRNN(nn.Module):\n#     def __init__(self, hidden_size, output_size,drop_out,num_layers,batchsize,embeddingsize,modelname):\n#         super(AttnDecoderRNN, self).__init__()\n#         maxlength = imaxlength\n#         self.hidden_size = hidden_size\n#         self.batchsize = batchsize\n#         self.num_layers = num_layers\n#         self.drop_out = drop_out\n#         self.embeddingsize = embeddingsize\n#         self.embedding = nn.Embedding(output_size, embeddingsize)\n#         self.modelname = modelname\n#         if modelname == 'RNN':\n#             self.model = nn.RNN(embeddingsize, hidden_size,num_layers)\n#         elif modelname == 'GRU':\n#             self.model = nn.GRU(embeddingsize, hidden_size,num_layers)\n#         elif modelname == 'LSTM':\n#             self.model = nn.LSTM(embeddingsize, hidden_size,num_layers)\n# #         self.gru = nn.GRU(embeddingsize, hidden_size,num_layers,dropout = drop_out,bidirectional = False)\n#         self.dropout = nn.Dropout(self.drop_out)\n#         self.out = nn.Linear(hidden_size, output_size)\n#         self.softmax = nn.LogSoftmax(dim=1)\n#         self.attn = nn.Linear(self.hidden_size + self.embeddingsize, maxlength).to(device)\n#         self.attn_combine = nn.Linear(self.hidden_size + self.embeddingsize, self.embeddingsize).to(device)\n#     def forward(self, input, encoder_hidden,encoder_output):\n        \n#         (hidden, cell) = encoder_hidden\n#         output = self.embedding(input).view(-1, self.batchsize, self.embeddingsize)\n#         output = self.dropout(output)\n#         cat = torch.cat((output[0], hidden[0]), 1)\n#         attn_weights = F.softmax(\n#             self.attn(cat), dim=1)\n#         attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n#                                  torch.permute(encoder_output,(1,0,2)))\n\n#         output = torch.cat((output[0], attn_applied.squeeze(1)), 1)\n#         output = self.attn_combine(output).unsqueeze(0)\n\n        \n#         output = F.relu(output)\n#         if self.modelname == 'RNN':\n#             output, hidden = self.model(output, hidden)\n#         elif self.modelname == 'GRU':\n#             output, hidden = self.model(output, hidden)\n#         elif self.modelname == 'LSTM':\n#             output, (hidden,cell) = self.model(output, (hidden,cell))\n        \n#         output = self.softmax(self.out(output[0])) #dim = [128,67] before applying softmax\n#         return output, (hidden,cell),attn_weights","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:36.367599Z","iopub.execute_input":"2023-05-21T09:36:36.368474Z","iopub.status.idle":"2023-05-21T09:36:36.377307Z","shell.execute_reply.started":"2023-05-21T09:36:36.368430Z","shell.execute_reply":"2023-05-21T09:36:36.376310Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"'''\nThis is train function \nAt first a teacher forcing ratio is given , it is 0.5 here\ninput_tensor = one batch of tensor of source words . They are preprocessed .\n                so , every tensor in a batch is of same size as ,  every word ends by EOW token\n                and then they are padded to reach the maximum length of source word in that batch\n\ntarget_tensor = True target tensor words corresponding to the batch of input tensor . Like \n                input tensor these are also preprocessed , They end with EOW token and then\n                they are padded to reach the maximum length of words in that batch\nencoder = encoder model instance\ndecoder = decoder model instance\nencoder_optimizer = The optimizer used for the encoder ( Can be SGD , RMSprop , Adam , NAdam)\ndecoder_optimizer = The optimizer used for the decoder ( Can be SGD , RMSprop , Adam , NAdam)\ncriterion = It means The nn module's CrossEntropyLoss instance\nbatchsize = batchsize of the model (valid for both encoder and decoder)\nIt returns the loss in that batch and also the word level accuracy in that batch\n'''\n\nteacher_forcing_ratio = 0.5\n\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batchsize):\n    hidden = encoder.initHidden()\n    cell = hidden\n    encoder_hidden = (hidden, cell)\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n    # encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n    accu = 0\n    # for ei in range(input_length):\n    #     encoder_output, encoder_hidden = encoder(\n    #         input_tensor[ei], encoder_hidden)\n    #     encoder_outputs[ei] = encoder_output[0, 0]\n    # print(input_tensor.shape)\n#     print(input_tensor)\n    encoder_output, encoder_hidden = encoder(\n            input_tensor, encoder_hidden)\n    decoder_input = torch.tensor([SOW_token] * batchsize, device=device)\n    d_hidden = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n    d_cell = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n    (hidden, cell) = encoder_hidden \n    if encoder.num_layers != decoder.num_layers:\n        d_hidden[:,:,:] = hidden[encoder.num_layers - 1,:,:]\n        if cell != None:\n            d_cell[:,:,:] = cell[encoder.num_layers - 1,:,:]\n        else :\n            d_cell = cell\n    else :\n        d_hidden = hidden\n        d_cell = cell\n    if d_cell == None and decoder.modelname == 'LSTM':\n        d_cell = d_hidden\n    decoder_hidden = (d_hidden , d_cell)\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n    decoder_words = []\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden ,attn_weights= decoder(\n                decoder_input, decoder_hidden,encoder_output)\n            \n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n            topv, topi = decoder_output.topk(1)\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n#             decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n      \n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden,attn_weights = decoder(\n                decoder_input, decoder_hidden ,encoder_output)\n            loss += criterion(decoder_output, target_tensor[di])\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n#             decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n            \n       \n    loss.backward()\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    accu = accuracy(decoder_words,target_tensor)\n    return loss.item() / target_length,accu","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:38.658940Z","iopub.execute_input":"2023-05-21T09:36:38.659313Z","iopub.status.idle":"2023-05-21T09:36:38.677013Z","shell.execute_reply.started":"2023-05-21T09:36:38.659282Z","shell.execute_reply":"2023-05-21T09:36:38.675866Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"'''\nencoder = encoder model instance\ndecoder = decoder model instance\nn_iters = total number of iteration of training on the whole training set or, total number of epochs\nlearning_rate = learning_rate of the optimizer model\nbatchsize = batchsize of the model\noptimizername = name of the optimizer  model used \nbeta = used in SGD for momentum hyper parameter\n\nThis function takes both source and target words , process them (changes the characters to integers) and pads \niff necessery to make each word of same length in a batch \nThen it runs total n_iters time \nIn each of them it trains each of the batch of training set and prints the training loss \n, validation loss , training accuracy , validation accuracy(after completion of an epoch)\n\n'''\ndef trainIters(encoder, decoder, n_iters, learning_rate,batchsize,optimizername,beta):\n    loss_total = 0  # Reset every print_every\n    accu_total = 0\n    if optimizername == 'Adam':\n        encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'NAdam':\n        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'RMSprop':\n        encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'SGD':\n        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate,momentum = beta)\n        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate,momentum = beta)\n    criterion = nn.CrossEntropyLoss()\n    \n    input_batch = []\n    output_batch = []\n    input_tensor = [tensorFromWord(lang_input,word) for word in english_words_train]\n    output_tensor = [tensorFromWord(lang_output,word) for word in hindi_words_train]\n\n    \n    \n    for i in range(0,len(english_words_train),batchsize):\n        if i + batchsize > len(english_words_train):\n            break\n        input_batch.append(nn.utils.rnn.pad_sequence(input_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n        output_batch.append(nn.utils.rnn.pad_sequence(output_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batch\n    \n\n\n    for iter in range(1, n_iters + 1):\n        for batchnum in range(len(input_batch)):\n#             print(input_batch[batchnum])\n            loss,accu = train(input_batch[batchnum], output_batch[batchnum], encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batchsize)\n            if batchnum % 500 == 0:\n                print(loss,batchnum)\n                pass\n            loss_total += loss * batchsize\n            accu_total = accu_total + accu\n        training_loss = loss_total/len(english_words_train)\n        training_accuracy = (accu_total * 100)/(len(input_batch) * batchsize)\n        validation_loss , validation_accuracy = evaluate(encoder,decoder,df_valid,batchsize)\n        print(\"training loss after epoch no {}\".format(iter) ,training_loss)\n        print(\"training accuracy after epoch no {}\".format(iter),training_accuracy)\n        print(\"validation loss after epoch no {}\".format(iter),validation_loss)\n        print(\"validation accuracy after epoch no {}\".format(iter),validation_accuracy)\n#         wandb.log({\"training_accuracy\": training_accuracy, \"validation_accuracy\": validation_accuracy, \"training_loss\": training_loss, \"validation_loss\": validation_loss,\"Epoch\" : iter})\n\n        loss_total = 0\n        accu_total = 0","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:55.187556Z","iopub.execute_input":"2023-05-21T09:36:55.187901Z","iopub.status.idle":"2023-05-21T09:36:55.205271Z","shell.execute_reply.started":"2023-05-21T09:36:55.187872Z","shell.execute_reply":"2023-05-21T09:36:55.204314Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"'''\nIt is the method which given predictedlist and actuallist returns accuracy\npredictedlist = predicted word list(target word)\nactuallist = actual word list(target word)\nIt calculates word level accuracy\n'''\n\ndef accuracy(predictedlist, actuallist):\n    accu = 0\n    N = actuallist.size()[1]\n    L = actuallist.size()[0]\n    predictedtensor = torch.zeros(L,N).to(device)\n    for i in range(L):\n        predictedtensor[i] = predictedlist[i]\n#     print(predictedtensor)\n    for i in range(N):\n        if torch.equal(predictedtensor[:,i],actuallist[:,i]) == True:\n            accu += 1\n        \n    return accu","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:56.885124Z","iopub.execute_input":"2023-05-21T09:36:56.885844Z","iopub.status.idle":"2023-05-21T09:36:56.893695Z","shell.execute_reply.started":"2023-05-21T09:36:56.885802Z","shell.execute_reply":"2023-05-21T09:36:56.892610Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"'''\nencoder = encoder model instance\ndecoder = decoder model instance\ndataset = dataset on which to evaluate the model\nbatchsize = number of words in a batch\ncalculates loss and accuracy of the model on the given dataset\n'''\n\ndef evaluate(encoder,decoder,dataset,batchsize):\n    loss_total = 0\n    accu_total = 0\n    input_batch = []\n    target_batch = []\n    encoder.eval()\n    decoder.eval()\n    criterion = nn.CrossEntropyLoss()\n    result = []\n    english_words = list(dataset['X'])\n    hindi_words = list(dataset['y'])\n    \n    \n    input_tensor = [tensorFromWord(lang_input,word) for word in english_words]\n    output_tensor = [tensorFromWord(lang_output,word) for word in hindi_words]\n\n    \n    \n    for i in range(0,len(english_words),batchsize):\n        if i + batchsize > len(english_words):\n            break\n        input_batch.append(nn.utils.rnn.pad_sequence(input_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n        target_batch.append(nn.utils.rnn.pad_sequence(output_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batch\n    \n    \n    loss = 0\n    for batchnum in range(len(input_batch)):\n        hidden = encoder.initHidden()\n        cell = hidden\n        encoder_hidden = (hidden, cell)\n    \n        input_length = input_batch[batchnum].size(0)\n        target_length = target_batch[batchnum].size(0)\n    \n        encoder_output, encoder_hidden = encoder(\n            input_batch[batchnum], encoder_hidden)\n        decoder_input = torch.tensor([SOW_token] * batchsize, device=device)\n\n        \n        \n        d_hidden = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n        d_cell = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n        (hidden, cell) = encoder_hidden \n        if encoder.num_layers != decoder.num_layers:\n            d_hidden[:,:,:] = hidden[encoder.num_layers - 1,:,:]\n            if cell != None:\n                d_cell[:,:,:] = cell[encoder.num_layers - 1,:,:]\n            else :\n                d_cell = cell\n        else :\n            d_hidden = hidden\n            d_cell = cell\n        if d_cell == None and decoder.modelname == 'LSTM':\n            d_cell = d_hidden\n        decoder_hidden = (d_hidden , d_cell)\n\n        decoder_words = []\n        res_words = []\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden,attn_weights = decoder(\n                decoder_input, decoder_hidden,encoder_output)\n            loss += criterion(decoder_output, target_batch[batchnum][di])\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n            res_words.append(topi.squeeze().detach().view(-1,1))\n    #       decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n        accu = accuracy(decoder_words,target_batch[batchnum])\n    \n        loss = loss.item() / target_length\n        loss_total += loss * batchsize\n        accu_total += accu\n\n        for i in range(batchsize):\n            resultantlist = []\n            for j in range(target_length):\n                if res_words[j][i].item() == 1:\n                    break\n                resultantlist.append(lang_output.index2char[res_words[j][i].item()])\n            result.append(resultantlist)\n    loss_total = loss_total/len(english_words)\n    accu_total = (accu_total * 100)/(len(input_batch) * batchsize)\n    print(\"loss = \", loss_total)\n    print(accu_total)\n#     finalresult = []\n#     for i in range(len(result)):\n#         finalresult.append(''.join(result[i]))\n#     print(dataset['y'][1])\n#     print(len(finalresult))\n#     correct = 0\n#     for i in range(len(finalresult)):\n#         if finalresult[i] == dataset['y'][i]:\n#             correct += 1\n#     print(correct)\n    \n#     with open('output.txt' , 'w') as fp:\n#         fp.write(\"correct output    \")\n#         fp.write(\"predicted output\\n\")\n#         fp.write(\"--------------------------------------------\\n\")\n#         for i in range(len(finalresult)):\n#             fp.write(\"--------------------------------------------\\n\")\n#             fp.write(f\"{dataset['y'][i]:<40}{finalresult[i]:<50}\")\n#             fp.write('\\n')\n#             fp.write(\"--------------------------------------------\\n\")\n# #             fp.write(\"%s\\n\"%finalresult[i])\n#     fp.close()\n    \n    \n#     vowelerror = 0\n#     consonanterror = 0\n#     vowelcorrect = 0\n#     consonantcorrect = 0\n#     for i in range(len(result)):\n#         truelist = list(dataset['y'][i])\n#         for j in range(min(len(truelist) , len(result[i]))):\n#             if is_vowel(truelist[j] , lang):\n#                 if  truelist[j] != result[i][j]:\n#                     vowelerror += 1\n#                 else:\n#                     vowelcorrect += 1\n#             elif is_consonant(truelist[j] , lang):\n#                 if  truelist[j] != result[i][j]:\n#                     consonanterror += 1\n#                 else:\n#                     consonantcorrect += 1\n#         if len(truelist) > len(result[i]):\n#             for j in range(len(result[i]) , len(truelist)):\n#                 if is_vowel(truelist[j] ,lang):\n#                     vowelerror += 1\n#                 else:\n#                     consonanterror += 1\n    \n    \n#     print(vowelerror)\n#     print(vowelcorrect)\n#     print(consonanterror)\n#     print(consonantcorrect)\n    encoder.train()\n    decoder.train()\n    return loss_total,accu_total","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:36:58.565561Z","iopub.execute_input":"2023-05-21T09:36:58.565922Z","iopub.status.idle":"2023-05-21T09:36:58.591910Z","shell.execute_reply.started":"2023-05-21T09:36:58.565891Z","shell.execute_reply":"2023-05-21T09:36:58.590783Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"encoder1 = EncoderRNN(lang_input.n_chars, 512,0.2,2,128,256,True,'GRU').to(device)\ndecoder1 = DecoderRNN(512, lang_output.n_chars,0.2,3,128,256,'LSTM').to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:37:00.279272Z","iopub.execute_input":"2023-05-21T09:37:00.279921Z","iopub.status.idle":"2023-05-21T09:37:00.423701Z","shell.execute_reply.started":"2023-05-21T09:37:00.279887Z","shell.execute_reply":"2023-05-21T09:37:00.422574Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"trainIters(encoder1,decoder1,2,0.001,128,'Adam',0)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:37:01.932282Z","iopub.execute_input":"2023-05-21T09:37:01.932717Z","iopub.status.idle":"2023-05-21T09:37:10.585929Z","shell.execute_reply.started":"2023-05-21T09:37:01.932686Z","shell.execute_reply":"2023-05-21T09:37:10.584571Z"},"trusted":true},"execution_count":129,"outputs":[{"name":"stdout","text":"4.1874995512120865 0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[129], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[125], line 52\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, learning_rate, batchsize, optimizername, beta)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_iters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batchnum \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_batch)):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#             print(input_batch[batchnum])\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m             loss,accu \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatchnum\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatchnum\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m batchnum \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(loss,batchnum)\n","Cell \u001b[0;32mIn[123], line 88\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batchsize)\u001b[0m\n\u001b[1;32m     86\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     87\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 88\u001b[0m accu \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m target_length,accu\n","Cell \u001b[0;32mIn[126], line 17\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(predictedlist, actuallist)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     print(predictedtensor)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m---> 17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictedtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactuallist\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m             accu \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accu\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"sweep_configuration = {\n    'method': 'bayes',\n    'name': 'ACCURACY VS EPOCH',\n    'metric': {\n        'goal': 'maximize', \n        'name': 'validation_accuracy'\n        },\n    'parameters': {\n        'embeddingsize' : {'values' : [128,256,512]},\n        'number_of_encoder_layers' : {'values' : [2,3]},\n        'number_of_decoder_layers' : {'values' : [2,3]},\n        'hidden_size' : {'values' : [256,512]},\n\n        'learning_rate': {'values':[0.0001,0.001]},\n        'beta' : {'values' : [0,0.9]},\n        'optimizer' : {'values' : ['SGD','Adam','NAdam','RMSprop']},\n        \n        'batchsize': {'values': [128,256]},\n        'bidirectional' : {'values' : [True , False]},\n        'n_iters': {'values': [20,15]},\n        'loss' : {'values' : ['cross_entropy']},\n        'encodermodelname' : {'values' : ['RNN','GRU','LSTM']},\n        'decodermodelname' : {'values' : ['RNN','GRU','LSTM']},\n        'drop_out' : {'values' : [0,0.2,0.3]}\n       }\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def withoutattention():\n    '''\n    main function which runs sweep\n    '''\n    \n    wandb.init(project = 'Assignment 2')\n    config = wandb.config\n#     print(config.hidden_size,\"HHHHHHHHHHHHHHHHHHHHHHHHHHH\")\n    encoder = EncoderRNN(lang_input.n_chars,config.hidden_size ,config.drop_out,config.number_of_encoder_layers,config.batchsize,config.embeddingsize,config.bidirectional,config.encodermodelname).to(device)\n    decoder = DecoderRNN(config.hidden_size,lang_output.n_chars ,config.drop_out,config.number_of_decoder_layers,config.batchsize,config.embeddingsize,config.decodermodelname).to(device)\n    run_name = \"es_{}_nenl_{}_ndel_{}_hs_{}_lr_{}_bt_{}_o_{}_bs_{}_bi_{}_n_iters_{}_loss_{}_emn_{}_dmn_{}_do_{}\".format(config.embeddingsize,config.number_of_encoder_layers,config.number_of_decoder_layers,config.hidden_size,config.learning_rate,config.beta,config.optimizer,config.batchsize,config.bidirectional,config.n_iters,config.loss,config.encodermodelname,config.decodermodelname,config.drop_out)\n    print(\"run name - \", run_name)\n    wandb.run.name = run_name\n    trainIters(encoder, decoder, config.n_iters, config.learning_rate, config.batchsize,config.optimizer,config.beta)\n    print(\"AFTER TRAINING - \")\n    print(\"validation accuracy\" , evaluate(encoder,decoder,df_valid,config.batchsize))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep = sweep_configuration , project = 'Assignment 2')\n\nwandb.agent(sweep_id , function = withoutattention , count = 30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nencoder = encoder model instance\ndecoder = decoder model instance\ndataset = dataset on which to evaluate the model\nbatchsize = number of words in a batch\ncalculates loss and accuracy of the model on the given dataset\n'''\n\ndef evaluatetest(encoder,decoder,dataset,batchsize):\n    loss_total = 0\n    accu_total = 0\n    input_batch = []\n    target_batch = []\n    encoder.eval()\n    decoder.eval()\n    criterion = nn.CrossEntropyLoss()\n    result = []\n    english_words = list(dataset['X'])\n    hindi_words = list(dataset['y'])\n    input_tensor = [tensorFromWord(lang_input,word) for word in english_words]\n    output_tensor = [tensorFromWord(lang_output,word) for word in hindi_words]\n\n    \n    \n    for i in range(0,len(english_words),batchsize):\n        if i + batchsize > len(english_words):\n            break\n        input_batch.append(nn.utils.rnn.pad_sequence(input_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n        target_batch.append(nn.utils.rnn.pad_sequence(output_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batch\n    \n    \n    loss = 0\n    for batchnum in range(len(input_batch)):\n        hidden = encoder.initHidden()\n        cell = hidden\n        encoder_hidden = (hidden, cell)\n    \n        input_length = input_batch[batchnum].size(0)\n        target_length = target_batch[batchnum].size(0)\n    \n        encoder_output, encoder_hidden = encoder(\n            input_batch[batchnum], encoder_hidden)\n        decoder_input = torch.tensor([SOW_token] * batchsize, device=device)\n\n        \n        \n        d_hidden = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n        d_cell = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n        (hidden, cell) = encoder_hidden \n        if encoder.num_layers != decoder.num_layers:\n            d_hidden[:,:,:] = hidden[encoder.num_layers - 1,:,:]\n            if cell != None:\n                d_cell[:,:,:] = cell[encoder.num_layers - 1,:,:]\n            else :\n                d_cell = cell\n        else :\n            d_hidden = hidden\n            d_cell = cell\n        if d_cell == None and decoder.modelname == 'LSTM':\n            d_cell = d_hidden\n        decoder_hidden = (d_hidden , d_cell)\n\n        decoder_words = []\n        res_words = []\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden,attn_weights = decoder(\n                decoder_input, decoder_hidden,encoder_output)\n            loss += criterion(decoder_output, target_batch[batchnum][di])\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n            res_words.append(topi.squeeze().detach().view(-1,1))\n    #       decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n        accu = accuracy(decoder_words,target_batch[batchnum])\n    \n        loss = loss.item() / target_length\n        loss_total += loss * batchsize\n        accu_total += accu\n\n        for i in range(batchsize):\n            resultantlist = []\n            for j in range(target_length):\n                if res_words[j][i].item() == 1:\n                    break\n                resultantlist.append(lang_output.index2char[res_words[j][i].item()])\n            result.append(resultantlist)\n    loss_total = loss_total/len(english_words)\n    accu_total = (accu_total * 100)/(len(input_batch) * batchsize)\n    print(\"loss = \", loss_total)\n    print('accuracy = ', accu_total)\n    finalresult = []\n    for i in range(len(result)):\n        finalresult.append(''.join(result[i]))\n\n    correct = 0\n    mydict = {}\n    for i in range(len(finalresult)):\n        if finalresult[i] == dataset['y'][i]:\n            correct += 1\n        elif len(dataset['y'][i]) not in mydict:\n            mydict[len(dataset['y'][i])] = 1\n        else:\n            mydict[len(dataset['y'][i])] += 1\n    print('words printed correclty - ' , correct)\n\n    keys = list(mydict.keys())\n    length = [x for x in keys]\n#     print(keys)\n    error = [mydict[x] for x in keys]\n    plt.scatter(length,error)\n    plt.xlabel('length')\n    plt.ylabel('error')\n    plt.show()\n    \n    with open('output.txt' , 'w') as fp:\n        fp.write(\"correct output    \")\n        fp.write(\"predicted output\\n\")\n        fp.write(\"--------------------------------------------\\n\")\n        for i in range(len(finalresult)):\n            fp.write(\"--------------------------------------------\\n\")\n            fp.write(f\"{dataset['y'][i]:<40}{finalresult[i]:<50}\")\n            fp.write('\\n')\n            fp.write(\"--------------------------------------------\\n\")\n    fp.close()\n    \n    \n    vowelerror = 0\n    consonanterror = 0\n    vowelcorrect = 0\n    consonantcorrect = 0\n    actualvowel = 0\n    actualconsonant = 0\n    for i in range(len(result)):\n        truelist = list(dataset['y'][i])\n        for j in range(min(len(truelist) , len(result[i]))):\n            if is_vowel(truelist[j] , lang):\n                actualvowel += 1\n            elif is_consonant(truelist[j] , lang):\n                actualconsonant += 1\n            if is_vowel(truelist[j] , lang):\n                if  truelist[j] != result[i][j]:\n                    vowelerror += 1\n                else:\n                    vowelcorrect += 1\n            elif is_consonant(truelist[j] , lang):\n                if  truelist[j] != result[i][j]:\n                    consonanterror += 1\n                else:\n                    consonantcorrect += 1\n        if len(truelist) > len(result[i]):\n            for j in range(len(result[i]) , len(truelist)):\n                if is_vowel(truelist[j] , lang):\n                    actualvowel += 1\n                elif is_consonant(truelist[j] , lang):\n                    actualconsonant += 1\n                if is_vowel(truelist[j] ,lang):\n                    vowelerror += 1\n                else:\n                    consonanterror += 1\n    \n    \n    print('vowel error = ',vowelerror)\n    print('correct vowel = ' , vowelcorrect)\n    print('consonant error' , consonanterror)\n    print('consonant correct' , consonantcorrect)\n    \n    \n    from sklearn import metrics\n    actual = []\n    predicted = []\n    for i in range(vowelcorrect):\n        actual.append('Vowel')\n        predicted.append('Vowel')\n    for i in range(consonantcorrect):\n        actual.append('Consonant')\n        predicted.append('Consonant')\n    for i in range(vowelerror):\n        actual.append('Vowel')\n        predicted.append('Consonant')\n    for i in range(consonanterror):\n        actual.append('Consonant')\n        predicted.append('Vowel')\n    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n\n    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Consonant', 'Vowel'])\n\n    cm_display.plot()\n    plt.show()\n    \n    print('error when actual character is consonant but predicted is vowel', consonanterror/actualconsonant)\n    print('error when actual character is vowel but predicted is consonant', vowelerror/actualvowel)\n    \n    \n    encoder.train()\n    decoder.train()\n    return loss_total,accu_total","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:04:32.592565Z","iopub.execute_input":"2023-05-21T11:04:32.593129Z","iopub.status.idle":"2023-05-21T11:04:32.634125Z","shell.execute_reply.started":"2023-05-21T11:04:32.593081Z","shell.execute_reply":"2023-05-21T11:04:32.633144Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# encoder1 = EncoderRNN(lang_input.n_chars, 512,0.3,2,256,512,True,'LSTM').to(device)\n# decoder1 = DecoderRNN(512, lang_output.n_chars,0.3,3,256,512,'LSTM').to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:14:59.569507Z","iopub.execute_input":"2023-05-21T09:14:59.569868Z","iopub.status.idle":"2023-05-21T09:14:59.575002Z","shell.execute_reply.started":"2023-05-21T09:14:59.569838Z","shell.execute_reply":"2023-05-21T09:14:59.574035Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"es_512_nenl_3_ndel_1_hs_1024_lr_0.001_bt_0_o_Adam_bs_128_bi_True_n_iters_30_loss_cross_entropy_emn_LSTM_dmn_LSTM_do_0.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best configuration\nembedding_size = 512\nnum_encoder_layers = 3\nnum_decoder_layers = 1\nhidden_size = 1024\nlearning_rate = 0.001\nbeta = 0\noptimizer = 'Adam'\nbatchsize = 128\nbidirectional = True\nn_iters = 30\nloss = 'cross_entropy'\nencodermodelname = 'LSTM'\ndecodermodelname = 'LSTM'\ndrop_out = 0.2","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:37:42.917460Z","iopub.execute_input":"2023-05-21T09:37:42.917835Z","iopub.status.idle":"2023-05-21T09:37:42.924454Z","shell.execute_reply.started":"2023-05-21T09:37:42.917806Z","shell.execute_reply":"2023-05-21T09:37:42.923398Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"encoder1 = EncoderRNN(lang_input.n_chars,hidden_size ,drop_out,num_encoder_layers,batchsize,embedding_size,bidirectional,encodermodelname).to(device)\ndecoder1 = DecoderRNN(hidden_size, lang_output.n_chars,drop_out,num_decoder_layers,batchsize,embedding_size,decodermodelname).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T09:37:45.231573Z","iopub.execute_input":"2023-05-21T09:37:45.232071Z","iopub.status.idle":"2023-05-21T09:37:45.991710Z","shell.execute_reply.started":"2023-05-21T09:37:45.232028Z","shell.execute_reply":"2023-05-21T09:37:45.990776Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"trainIters(encoder1,decoder1,n_iters,learning_rate,batchsize,optimizer,beta)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:10:59.371013Z","iopub.execute_input":"2023-05-21T11:10:59.371548Z","iopub.status.idle":"2023-05-21T11:10:59.380773Z","shell.execute_reply.started":"2023-05-21T11:10:59.371506Z","shell.execute_reply":"2023-05-21T11:10:59.379593Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"evaluatetest(encoder1,decoder1,df_test,batchsize)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:04:35.090704Z","iopub.execute_input":"2023-05-21T11:04:35.091087Z","iopub.status.idle":"2023-05-21T11:04:43.172188Z","shell.execute_reply.started":"2023-05-21T11:04:35.091056Z","shell.execute_reply":"2023-05-21T11:04:43.171130Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"loss =  0.927897446284371\naccuracy =  37.4755859375\nwords printed correclty -  1535\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"vowel error =  341\ncorrect vowel =  615\nconsonant error 4234\nconsonant correct 12041\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"error when actual character is consonant but predicted is vowel 0.26804254241580144\nerror when actual character is vowel but predicted is consonant 0.35669456066945604\n","output_type":"stream"},{"execution_count":139,"output_type":"execute_result","data":{"text/plain":"(0.927897446284371, 37.4755859375)"},"metadata":{}}]}]}