{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\ninstalling wandb and indic package\n'''\n!pip install wandb\n!pip install indic-nlp-library","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-20T14:18:17.407360Z","iopub.execute_input":"2023-05-20T14:18:17.408082Z","iopub.status.idle":"2023-05-20T14:18:46.382969Z","shell.execute_reply.started":"2023-05-20T14:18:17.408040Z","shell.execute_reply":"2023-05-20T14:18:46.381575Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.0)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.28.2)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.4)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.20.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting indic-nlp-library\n  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.23.5)\nCollecting morfessor\n  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.5.3)\nRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.2.4)\nCollecting sphinx-argparse\n  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.3)\nCollecting sphinx>=1.2.0\n  Downloading sphinx-7.0.1-py3-none-any.whl (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->indic-nlp-library) (1.16.0)\nCollecting sphinxcontrib-htmlhelp>=2.0.0\n  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.12.1)\nRequirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.28.2)\nRequirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\nCollecting imagesize>=1.3\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nCollecting sphinxcontrib-devhelp\n  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.19)\nCollecting alabaster<0.8,>=0.7\n  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\nCollecting sphinxcontrib-qthelp\n  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinxcontrib-applehelp\n  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinxcontrib-serializinghtml>=1.1.5\n  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinxcontrib-jsmath\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\nRequirement already satisfied: Pygments>=2.13 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.1)\nInstalling collected packages: morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, alabaster, sphinx, sphinx-argparse, indic-nlp-library\nSuccessfully installed alabaster-0.7.13 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-7.0.1 sphinx-argparse-0.4.0 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"'''\nall important libraries are called\n'''\nfrom indicnlp.langinfo import *\nlang = 'hi'\nimport wandb\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist, fashion_mnist\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport shutil\nimport torch.nn.functional as F\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:18:46.385729Z","iopub.execute_input":"2023-05-20T14:18:46.386145Z","iopub.status.idle":"2023-05-20T14:18:58.508087Z","shell.execute_reply.started":"2023-05-20T14:18:46.386103Z","shell.execute_reply":"2023-05-20T14:18:58.506985Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\ndevice variable is set\n'''\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:18:58.509660Z","iopub.execute_input":"2023-05-20T14:18:58.510446Z","iopub.status.idle":"2023-05-20T14:18:58.580838Z","shell.execute_reply.started":"2023-05-20T14:18:58.510390Z","shell.execute_reply":"2023-05-20T14:18:58.579800Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"'''\ntrain path , valid path , test path are written\n'''\ntrain_path = \"/kaggle/input/dataset/aksharantar_sampled/hin/hin_train.csv\"\nvalid_path = \"/kaggle/input/dataset/aksharantar_sampled/hin/hin_valid.csv\"\ntest_path = \"/kaggle/input/dataset/aksharantar_sampled/hin/hin_test.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:18:58.584956Z","iopub.execute_input":"2023-05-20T14:18:58.585603Z","iopub.status.idle":"2023-05-20T14:18:58.593225Z","shell.execute_reply.started":"2023-05-20T14:18:58.585563Z","shell.execute_reply":"2023-05-20T14:18:58.592365Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"'''\nthe train test validation set are these\n'''\ndf_train = pd.read_csv(train_path,names = ['X','y'])\ndf_valid = pd.read_csv(valid_path,names = ['X','y'])\ndf_test = pd.read_csv(test_path,names = ['X','y'])","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:22:43.634332Z","iopub.execute_input":"2023-05-20T14:22:43.634697Z","iopub.status.idle":"2023-05-20T14:22:43.764195Z","shell.execute_reply.started":"2023-05-20T14:22:43.634667Z","shell.execute_reply":"2023-05-20T14:22:43.763026Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"'''\ntarget word's maxlength(Hindi) and source word's maxlength(english) are these\n'''\ntargetmaxlen = 0\nsourcemaxlen = 0\nfor word in df_train.iloc[:,1]:\n    targetmaxlen = max(targetmaxlen,len(word))\nfor word in df_train.iloc[:,0]:\n    sourcemaxlen = max(sourcemaxlen,len(word))","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:23:45.753738Z","iopub.execute_input":"2023-05-20T14:23:45.754523Z","iopub.status.idle":"2023-05-20T14:23:45.821607Z","shell.execute_reply.started":"2023-05-20T14:23:45.754479Z","shell.execute_reply":"2023-05-20T14:23:45.820626Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"'''\nstart of the word and end of the word are declared\n'''\nSOW_token = 0\nEOW_token = 1\n\n'''\nLang class is created and two instances of it are created which are for source and target language\n'''\nclass Lang:\n    def __init__(self):\n        self.char2index = {}\n        self.index2char = {0: '0', 1: '1'} #'0' sow and '1' eow\n        self.n_chars = 2  # Count SOS and EOS\n\n\n    def addWord(self, word):\n        for c in word:\n            if c not in self.char2index:\n                self.char2index[c] = self.n_chars\n                self.index2char[self.n_chars] = c\n                self.n_chars += 1","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:24:43.344490Z","iopub.execute_input":"2023-05-20T14:24:43.344880Z","iopub.status.idle":"2023-05-20T14:24:43.351320Z","shell.execute_reply.started":"2023-05-20T14:24:43.344850Z","shell.execute_reply":"2023-05-20T14:24:43.350352Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"lang_input = Lang()\nlang_output = Lang()\n# li = list(df_train['X'])","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:24:49.250465Z","iopub.execute_input":"2023-05-20T14:24:49.250885Z","iopub.status.idle":"2023-05-20T14:24:49.258535Z","shell.execute_reply.started":"2023-05-20T14:24:49.250851Z","shell.execute_reply":"2023-05-20T14:24:49.257423Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"'''\nAll the unique character present in both source and target words are stored and given unique number \nTrain Test Valid set all are used to know the unique characters of the words \nOtherwise good mapping of characters to number would not be possible\n'''\n\nenglish_words_train = list(df_train['X'])\nfor word in english_words_train:\n    lang_input.addWord(word)\nhindi_words_train = list(df_train['y'])\nfor word in hindi_words_train:\n    lang_output.addWord(word)\n    \n    \nenglish_words_valid = list(df_valid['X'])\nfor word in english_words_valid:\n    lang_input.addWord(word)\nhindi_words_valid = list(df_valid['y'])\nfor word in hindi_words_valid:\n    lang_output.addWord(word)\n    \n    \nenglish_words_test = list(df_test['X'])\nfor word in english_words_test:\n    lang_input.addWord(word)\nhindi_words_test = list(df_test['y'])\nfor word in hindi_words_test:\n    lang_output.addWord(word)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:26:35.972812Z","iopub.execute_input":"2023-05-20T14:26:35.973175Z","iopub.status.idle":"2023-05-20T14:26:36.120155Z","shell.execute_reply.started":"2023-05-20T14:26:35.973146Z","shell.execute_reply":"2023-05-20T14:26:36.119180Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def indexesFromWord(lang, word):\n    li = []\n    for c in word:\n        li.append(lang.char2index[c])\n    return li\n\n\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:42:21.596693Z","iopub.execute_input":"2023-05-20T15:42:21.597696Z","iopub.status.idle":"2023-05-20T15:42:21.603919Z","shell.execute_reply.started":"2023-05-20T15:42:21.597659Z","shell.execute_reply":"2023-05-20T15:42:21.602964Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"'''\nEncoder class is here .\nIt extends nn of pytorch \ninput_size = Vocabulary size of source language(integer)\nhidden_size = number of neurons in the hidden layer of the encoder(integer)\ndrop_out = probability of a node being dropped out(number between 0 to 1)\nnum_layers = How many layers does the encoder has , These layers are stacked, one above another(Integer)\nbatchsize = total number of word pair in a batch(Integer)\nembeddingsize = The input (source word) characters are each converted to some embedding \n                by passing through one embedding layer , this denotes the size of that \n                embedding layer(Integer)\nbidirectional = if the encoder is bidirectional or not (takes boolean True / False)\nmodelname = name of the model (can be RNN , GRU , LSTM )(str) \ninput = Batch of words or a single word\nencoder_hidden = a tuple containing ( hidden , cell ) , LSTM has cell , so I kept a cell \n                It is used when needed , hidden and cell are initialised with initHidden method\nreturns output , a tuple containing hidden and cell state\n'''\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size,drop_out,num_layers,batchsize,embeddingsize,bidirectional,modelname):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.batchsize = batchsize\n        self.drop_out = drop_out\n        self.num_layers = num_layers\n        self.embeddingsize = embeddingsize\n        self.embedding = nn.Embedding(input_size, embeddingsize)\n        self.bidirectional = bidirectional\n        self.D = 1\n        if self.bidirectional == True:\n            self.D = 2\n        self.modelname = modelname\n        if modelname == 'RNN':\n            self.model = nn.RNN(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n        elif modelname == 'GRU':\n            self.model = nn.GRU(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n        elif modelname == 'LSTM':\n            self.model = nn.LSTM(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n\n        self.dropout = nn.Dropout(self.drop_out)\n\n    def forward(self, input, encoder_hidden):\n        embedded = self.embedding(input).view(-1,self.batchsize,self.embeddingsize)\n        maxlength = embedded.size()[0]\n        (hidden ,cell) = encoder_hidden\n        output = self.dropout(embedded)\n\n    \n        if self.modelname == 'RNN':\n            output, hidden = self.model(output, hidden)\n            cell = None\n        elif self.modelname == 'GRU':\n            output, hidden = self.model(output, hidden)\n            cell = None\n        elif self.modelname == 'LSTM':\n            output, (hidden,cell) = self.model(output, (hidden,cell))\n\n        if self.D == 2:\n            h = 0\n            o = 0\n            newoutput = torch.zeros(maxlength, self.batchsize, self.hidden_size, device = device)\n            newhidden = torch.zeros(self.num_layers, self.batchsize, self.hidden_size, device = device)\n            for i in range(0,self.D * self.num_layers,2):\n                newhidden[h, : , :] = torch.div(hidden[i, :, :].add(hidden[i + 1, :, :]),2)\n                h += 1\n            hidden = newhidden\n            for i in range(0,self.D * self.hidden_size,2):\n                newoutput[: , : ,o] = torch.div(output[: ,: ,i].add(output[: , : ,i + 1]),2)\n                o += 1\n            output = newoutput\n            if cell != None:\n                c = 0\n                newcell = torch.zeros(self.num_layers, self.batchsize, self.hidden_size, device = device)\n                for i in range(0,self.D * self.num_layers,2):\n                    newcell[c, : , :] = torch.div(cell[i, :, :].add(cell[i + 1, :, :]),2)\n                    c += 1\n                cell = newcell\n        return output, (hidden,cell)\n\n    def initHidden(self):\n        return torch.zeros(self.D * self.num_layers, self.batchsize, self.hidden_size, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:47:51.004380Z","iopub.execute_input":"2023-05-20T14:47:51.004748Z","iopub.status.idle":"2023-05-20T14:47:51.027792Z","shell.execute_reply.started":"2023-05-20T14:47:51.004719Z","shell.execute_reply":"2023-05-20T14:47:51.026952Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"'''\nDecoder class is here .\nIt extends nn of pytorch \nhidden_size = number of neurons in the hidden layer of the encoder(integer)\noutput_size = Vocabulary size of target language(integer)\ndrop_out = probability of a node being dropped out(number between 0 to 1)\nnum_layers = How many layers does the decoder has , These layers are stacked, one above another(Integer)\nbatchsize = total number of word pair in a batch(Integer)\nembeddingsize = The word (target word) characters are each converted to some embedding \n                by passing through one embedding layer , this denotes the size of that \n                embedding layer(Integer)\nmodelname = name of the model (can be RNN , GRU , LSTM )(str) \ninput = Batch of words or a single word or batch of words at a timestamp \nencoder_hidden = a tuple containing ( hidden , cell ) , LSTM has cell , so I kept a cell \n                It is used when needed , hidden and cell are initialised with initHidden method\nencoder_output = output of encoder (needed in attention implementation)\nmaxlength = max number of characters in a batch of target words\n\n\nreturns output , a tuple of hidden and cell state , attention weights (None as it is a normal decoder)\n'''\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size,drop_out,num_layers,batchsize,embeddingsize,modelname):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.batchsize = batchsize\n        self.num_layers = num_layers\n        self.drop_out = drop_out\n        self.embeddingsize = embeddingsize\n        self.embedding = nn.Embedding(output_size, embeddingsize)\n        self.modelname = modelname\n        if modelname == 'RNN':\n            self.model = nn.RNN(embeddingsize, hidden_size,num_layers)\n        elif modelname == 'GRU':\n            self.model = nn.GRU(embeddingsize, hidden_size,num_layers)\n        elif modelname == 'LSTM':\n            self.model = nn.LSTM(embeddingsize, hidden_size,num_layers)\n\n        self.dropout = nn.Dropout(self.drop_out)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, encoder_hidden,encoder_output,maxlength):\n        output = self.embedding(input).view(-1, self.batchsize, self.embeddingsize)\n        output = self.dropout(output)\n        output = F.relu(output)\n        (hidden, cell) = encoder_hidden\n        if self.modelname == 'RNN':\n            output, hidden = self.model(output, hidden)\n        elif self.modelname == 'GRU':\n            output, hidden = self.model(output, hidden)\n        elif self.modelname == 'LSTM':\n            output, (hidden,cell) = self.model(output, (hidden,cell))\n        \n        output = self.softmax(self.out(output[0]))#dim = [128,67] before applying softmax\n        return output, (hidden,cell),None","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:47:52.608555Z","iopub.execute_input":"2023-05-20T14:47:52.609310Z","iopub.status.idle":"2023-05-20T14:47:52.622714Z","shell.execute_reply.started":"2023-05-20T14:47:52.609271Z","shell.execute_reply":"2023-05-20T14:47:52.621253Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"'''\nThis is train function \nAt first a teacher forcing ratio is given , it is 0.5 here\ninput_tensor = one batch of tensor of source words . They are preprocessed .\n                so , every tensor in a batch is of same size as ,  every word ends by EOW token\n                and then they are padded to reach the maximum length of source word in that batch\n\ntarget_tensor = True target tensor words corresponding to the batch of input tensor . Like \n                input tensor these are also preprocessed , They end with EOW token and then\n                they are padded to reach the maximum length of words in that batch\nencoder = encoder model instance\ndecoder = decoder model instance\nencoder_optimizer = The optimizer used for the encoder ( Can be SGD , RMSprop , Adam , NAdam)\ndecoder_optimizer = The optimizer used for the decoder ( Can be SGD , RMSprop , Adam , NAdam)\ncriterion = It means The nn module's CrossEntropyLoss instance\nbatchsize = batchsize of the model (valid for both encoder and decoder)\nIt returns the loss in that batch and also the word level accuracy in that batch\n'''\n\nteacher_forcing_ratio = 0.5\n\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batchsize):\n    hidden = encoder.initHidden()\n    cell = hidden\n    encoder_hidden = (hidden, cell)\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n    # encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n    accu = 0\n    # for ei in range(input_length):\n    #     encoder_output, encoder_hidden = encoder(\n    #         input_tensor[ei], encoder_hidden)\n    #     encoder_outputs[ei] = encoder_output[0, 0]\n    # print(input_tensor.shape)\n#     print(input_tensor)\n    encoder_output, encoder_hidden = encoder(\n            input_tensor, encoder_hidden)\n    decoder_input = torch.tensor([SOW_token] * batchsize, device=device)\n    d_hidden = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n    d_cell = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n    (hidden, cell) = encoder_hidden \n    if encoder.num_layers != decoder.num_layers:\n        d_hidden[:,:,:] = hidden[encoder.num_layers - 1,:,:]\n        if cell != None:\n            d_cell[:,:,:] = cell[encoder.num_layers - 1,:,:]\n        else :\n            d_cell = cell\n    else :\n        d_hidden = hidden\n        d_cell = cell\n    if d_cell == None and decoder.modelname == 'LSTM':\n        d_cell = d_hidden\n    decoder_hidden = (d_hidden , d_cell)\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n    decoder_words = []\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden ,attn_weights= decoder(\n                decoder_input, decoder_hidden,encoder_output,input_tensor.size(0))\n            \n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n            topv, topi = decoder_output.topk(1)\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n#             decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n      \n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden,attn_weights = decoder(\n                decoder_input, decoder_hidden ,encoder_output,input_tensor.size(0))\n            loss += criterion(decoder_output, target_tensor[di])\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n#             decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n            \n       \n    loss.backward()\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    accu = accuracy(decoder_words,target_tensor)\n    return loss.item() / target_length,accu","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:24:41.145434Z","iopub.execute_input":"2023-05-20T15:24:41.145831Z","iopub.status.idle":"2023-05-20T15:24:41.162279Z","shell.execute_reply.started":"2023-05-20T15:24:41.145800Z","shell.execute_reply":"2023-05-20T15:24:41.161272Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"'''\nencoder = encoder model instance\ndecoder = decoder model instance\nn_iters = total number of iteration of training on the whole training set or, total number of epochs\nlearning_rate = learning_rate of the optimizer model\nbatchsize = batchsize of the model\noptimizername = name of the optimizer  model used \nbeta = used in SGD for momentum hyper parameter\n\nThis function takes both source and target words , process them (changes the characters to integers) and pads \niff necessery to make each word of same length in a batch \nThen it runs total n_iters time \nIn each of them it trains each of the batch of training set and prints the training loss \n, validation loss , training accuracy , validation accuracy(after completion of an epoch)\n\n'''\ndef trainIters(encoder, decoder, n_iters, learning_rate,batchsize,optimizername,beta):\n    loss_total = 0  # Reset every print_every\n    accu_total = 0\n    if optimizername == 'Adam':\n        encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'NAdam':\n        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'RMSprop':\n        encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'SGD':\n        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate,momentum = beta)\n        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate,momentum = beta)\n    criterion = nn.CrossEntropyLoss()\n    input_batch = []\n    output_batch = []\n    input_tensor = [tensorFromWord(lang_input,word) for word in english_words_train]\n    output_tensor = [tensorFromWord(lang_output,word) for word in hindi_words_train]\n#     print(input_tensor[0].size())\n    for i in range(0, len(english_words_train), batchsize):\n#         if i + batchsize >= len(english_words_train):\n#             break\n        input_batch.append(nn.utils.rnn.pad_sequence(input_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n        output_batch.append(nn.utils.rnn.pad_sequence(output_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n        # print(input_batch[-1].shape)\n        # print(output_batch[-1].shape)\n#     print(len(input_batch),batchsize)\n    for iter in range(1, n_iters + 1):\n        for batchnum in range(len(input_batch)):\n#             print(input_batch[batchnum])\n            loss,accu = train(input_batch[batchnum], output_batch[batchnum], encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batchsize)\n            if batchnum % 500 == 0:\n                print(loss,batchnum)\n                pass\n            loss_total += loss * batchsize\n            accu_total = accu_total + accu\n        training_loss = loss_total/len(english_words_train)\n        training_accuracy = (accu_total * 100)/(len(input_batch) * batchsize)\n        validation_loss , validation_accuracy = evaluate(encoder,decoder,df_valid,batchsize)\n        print(\"training loss after epoch no {}\".format(iter) ,training_loss)\n        print(\"training accuracy after epoch no {}\".format(iter),training_accuracy)\n        print(\"validation loss after epoch no {}\".format(iter),validation_loss)\n        print(\"validation accuracy after epoch no {}\".format(iter),validation_accuracy)\n#         wandb.log({\"training_accuracy\": training_accuracy, \"validation_accuracy\": validation_accuracy, \"training_loss\": training_loss, \"validation_loss\": validation_loss,\"Epoch\" : iter})\n\n        loss_total = 0\n        accu_total = 0","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:34:00.673066Z","iopub.execute_input":"2023-05-20T15:34:00.673481Z","iopub.status.idle":"2023-05-20T15:34:00.688567Z","shell.execute_reply.started":"2023-05-20T15:34:00.673450Z","shell.execute_reply":"2023-05-20T15:34:00.687648Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"'''\nIt is the method which given predictedlist and actuallist returns accuracy\npredictedlist = predicted word list(target word)\nactuallist = actual word list(target word)\nIt calculates word level accuracy\n'''\n\ndef accuracy(predictedlist, actuallist):\n    accu = 0\n    N = actuallist.size()[1]\n    L = actuallist.size()[0]\n    predictedtensor = torch.zeros(L,N).to(device)\n    for i in range(L):\n        predictedtensor[i] = predictedlist[i]\n#     print(predictedtensor)\n    for i in range(N):\n        if torch.equal(predictedtensor[:,i],actuallist[:,i]) == True:\n            accu += 1\n        \n    return accu","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:40:41.650211Z","iopub.execute_input":"2023-05-20T15:40:41.650603Z","iopub.status.idle":"2023-05-20T15:40:41.657140Z","shell.execute_reply.started":"2023-05-20T15:40:41.650574Z","shell.execute_reply":"2023-05-20T15:40:41.656201Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"'''\nencoder = encoder model instance\ndecoder = decoder model instance\ndataset = dataset on which to evaluate the model\nbatchsize = number of words in a batch\ncalculates loss and accuracy of the model on the given dataset\n'''\n\ndef evaluate(encoder,decoder,dataset,batchsize):\n    loss_total = 0\n    accu_total = 0\n    input_batch = []\n    target_batch = []\n    encoder.eval()\n    decoder.eval()\n    criterion = nn.CrossEntropyLoss()\n    result = []\n    english_words = list(dataset['X'])\n    hindi_words = list(dataset['y'])\n    input_tensor = [tensorFromWord(lang_input,word) for word in english_words]\n    output_tensor = [tensorFromWord(lang_output,word) for word in hindi_words]\n    \n    for i in range(0,len(english_words),batchsize):\n        if i + batchsize > len(english_words):\n            break\n        input_batch.append(nn.utils.rnn.pad_sequence(input_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n        target_batch.append(nn.utils.rnn.pad_sequence(output_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n    loss = 0\n    for batchnum in range(len(input_batch)):\n        hidden = encoder.initHidden()\n        cell = hidden\n        encoder_hidden = (hidden, cell)\n    \n        input_length = input_batch[batchnum].size(0)\n        target_length = target_batch[batchnum].size(0)\n    \n        encoder_output, encoder_hidden = encoder(\n            input_batch[batchnum], encoder_hidden)\n        decoder_input = torch.tensor([SOW_token] * batchsize, device=device)\n\n        \n        \n        d_hidden = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n        d_cell = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n        (hidden, cell) = encoder_hidden \n        if encoder.num_layers != decoder.num_layers:\n            d_hidden[:,:,:] = hidden[encoder.num_layers - 1,:,:]\n            if cell != None:\n                d_cell[:,:,:] = cell[encoder.num_layers - 1,:,:]\n            else :\n                d_cell = cell\n        else :\n            d_hidden = hidden\n            d_cell = cell\n        if d_cell == None and decoder.modelname == 'LSTM':\n            d_cell = d_hidden\n        decoder_hidden = (d_hidden , d_cell)\n\n        decoder_words = []\n        res_words = []\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden,attn_weights = decoder(\n                decoder_input, decoder_hidden,encoder_output,input_batch[batchnum].size(0))\n            loss += criterion(decoder_output, target_batch[batchnum][di])\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n            res_words.append(topi.squeeze().detach().view(-1,1))\n    #       decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n        accu = accuracy(decoder_words,target_batch[batchnum])\n    \n        loss = loss.item() / target_length\n        loss_total += loss * batchsize\n        accu_total += accu\n\n        for i in range(batchsize):\n            resultantlist = []\n            for j in range(target_length):\n                if res_words[j][i].item() == 1:\n                    break\n                resultantlist.append(lang_output.index2char[res_words[j][i].item()])\n            result.append(resultantlist)\n    loss_total = loss_total/len(english_words)\n    accu_total = (accu_total * 100)/(len(input_batch) * batchsize)\n    print(\"loss = \", loss_total)\n    print(accu_total)\n#     finalresult = []\n#     for i in range(len(result)):\n#         finalresult.append(''.join(result[i]))\n#     print(dataset['y'][1])\n#     print(len(finalresult))\n#     correct = 0\n#     for i in range(len(finalresult)):\n#         if finalresult[i] == dataset['y'][i]:\n#             correct += 1\n#     print(correct)\n    \n#     with open('output.txt' , 'w') as fp:\n#         fp.write(\"correct output    \")\n#         fp.write(\"predicted output\\n\")\n#         fp.write(\"--------------------------------------------\\n\")\n#         for i in range(len(finalresult)):\n#             fp.write(\"--------------------------------------------\\n\")\n#             fp.write(f\"{dataset['y'][i]:<40}{finalresult[i]:<50}\")\n#             fp.write('\\n')\n#             fp.write(\"--------------------------------------------\\n\")\n# #             fp.write(\"%s\\n\"%finalresult[i])\n#     fp.close()\n    \n    \n#     vowelerror = 0\n#     consonanterror = 0\n#     vowelcorrect = 0\n#     consonantcorrect = 0\n#     for i in range(len(result)):\n#         truelist = list(dataset['y'][i])\n#         for j in range(min(len(truelist) , len(result[i]))):\n#             if is_vowel(truelist[j] , lang):\n#                 if  truelist[j] != result[i][j]:\n#                     vowelerror += 1\n#                 else:\n#                     vowelcorrect += 1\n#             elif is_consonant(truelist[j] , lang):\n#                 if  truelist[j] != result[i][j]:\n#                     consonanterror += 1\n#                 else:\n#                     consonantcorrect += 1\n#         if len(truelist) > len(result[i]):\n#             for j in range(len(result[i]) , len(truelist)):\n#                 if is_vowel(truelist[j] ,lang):\n#                     vowelerror += 1\n#                 else:\n#                     consonanterror += 1\n    \n    \n#     print(vowelerror)\n#     print(vowelcorrect)\n#     print(consonanterror)\n#     print(consonantcorrect)\n    encoder.train()\n    decoder.train()\n    return loss_total,accu_total","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:40:43.058445Z","iopub.execute_input":"2023-05-20T15:40:43.058818Z","iopub.status.idle":"2023-05-20T15:40:43.081787Z","shell.execute_reply.started":"2023-05-20T15:40:43.058781Z","shell.execute_reply":"2023-05-20T15:40:43.080688Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# encoder1 = EncoderRNN(lang_input.n_chars, 512,0.2,2,128,256,True,'GRU').to(device)\n# decoder1 = DecoderRNN(512, lang_output.n_chars,0.2,3,128,256,'LSTM').to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:44:39.246638Z","iopub.execute_input":"2023-05-20T15:44:39.247110Z","iopub.status.idle":"2023-05-20T15:44:39.255417Z","shell.execute_reply.started":"2023-05-20T15:44:39.247068Z","shell.execute_reply":"2023-05-20T15:44:39.254283Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# trainIters(encoder1,decoder1,1,0.001,128,'Adam',0)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:44:40.212292Z","iopub.execute_input":"2023-05-20T15:44:40.212955Z","iopub.status.idle":"2023-05-20T15:44:40.218140Z","shell.execute_reply.started":"2023-05-20T15:44:40.212916Z","shell.execute_reply":"2023-05-20T15:44:40.216985Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}