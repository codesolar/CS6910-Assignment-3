{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy\nimport wandb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist, fashion_mnist\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport math\n'''\nargparse -- which helps in passing argument from CLI to this program\n'''\nimport argparse\nparser = argparse.ArgumentParser(description = 'training the neural network')","metadata":{"_uuid":"285cd3f0-0b40-4050-81d0-6a701e5b1f2c","_cell_guid":"208dea0c-dfb3-4fb5-b4b3-77d679ad97c1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ninstalling wandb and indic package\n'''\n!pip install wandb\n!pip install indic-nlp-library\n'''\nall important libraries are called\n'''\nfrom indicnlp.langinfo import *\nlang = 'hi'\nimport wandb\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist, fashion_mnist\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport shutil\nimport torch.nn.functional as F\nimport random","metadata":{"_uuid":"2d15770a-f34c-4396-8add-5e3f1dad608c","_cell_guid":"510c45b9-15d2-4e09-9ead-f49e0dd41f61","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description = 'training the seq2seq model')\nparser.add_argument('-wp','--wandb_project')\nparser.add_argument('-we','--wandb_entity')\nparser.add_argument('-d','--dataset')\nparser.add_argument('-es','--embeddingsize',type = int)\nparser.add_argument('-nenl','--number_of_encoder_layers',type = int)\nparser.add_argument('-ndel','--number_of_decoder_layers',type = int)\nparser.add_argument('-hs','--hiddensize',type = int)\nparser.add_argument('-lr','--learning_rate',type = float)\nparser.add_argument('-bt','--beta',type = float)\nparser.add_argument('-o','--optimizer')\nparser.add_argument('-bs','--batch_size',type = int)\nparser.add_argument('-bi','--bidirectional')\nparser.add_argument('-n_iters','--num_iteration',type = int)\nparser.add_argument('-loss','--loss')\nparser.add_argument('-emn','--encodermodelname')\nparser.add_argument('-dmn','--decodermodelname')\nparser.add_argument('-do','--drop_out',type = float)\nparser.add_argument('-attn','--attention')\nargs = parser.parse_args()","metadata":{"_uuid":"f6a2e4af-8d67-4136-a57b-0d2557a71f39","_cell_guid":"ea49de0a-fc32-44cc-b8c4-66ce8755b4d8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndevice variable is set\n'''\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"9ac54371-ba1f-4e0f-9b87-5e4ea3964885","_cell_guid":"9f59fb44-c9bb-4849-8b08-52f2f2c96472","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataname = 'hin'\nif args.dataset is not None:\n    dataname = args.dataset","metadata":{"_uuid":"329c30de-14ef-4c40-a344-2c20a2150ea8","_cell_guid":"10541d3d-0eb3-4fd0-9e6e-1be486d1a4b7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntrain path , valid path , test path are written\n'''\ntrain_path = '/kaggle/input/dataset/aksharantar_sampled/' + dataname + '/' + dataname + '_train.csv'\nvalid_path = '/kaggle/input/dataset/aksharantar_sampled/'+ dataname + '/' + dataname +'_valid.csv'\ntest_path = '/kaggle/input/dataset/aksharantar_sampled/' + dataname + '/' + dataname +'_test.csv'","metadata":{"_uuid":"93241645-2ab3-4424-bc48-d7874996c5e3","_cell_guid":"cd92abfd-3e52-4c51-a58d-83176cec29f1","collapsed":false,"execution":{"iopub.status.busy":"2023-05-21T15:39:13.913788Z","iopub.execute_input":"2023-05-21T15:39:13.914149Z","iopub.status.idle":"2023-05-21T15:39:13.919839Z","shell.execute_reply.started":"2023-05-21T15:39:13.914120Z","shell.execute_reply":"2023-05-21T15:39:13.918790Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nthe train test validation set are these\n'''\ndf_train = pd.read_csv(train_path,names = ['X','y'])\ndf_valid = pd.read_csv(valid_path,names = ['X','y'])\ndf_test = pd.read_csv(test_path,names = ['X','y'])","metadata":{"_uuid":"39332451-6bfc-4b5f-91f9-a3efe228eb83","_cell_guid":"3bff8e1e-5afd-4386-b4b5-325aaeb1bc9b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntarget word's maxlength(Hindi) and source word's maxlength(english) are these\n'''\ntargetmaxlen = 0\nsourcemaxlen = 0\nfor word in df_train.iloc[:,1]:\n    targetmaxlen = max(targetmaxlen,len(word))\nfor word in df_train.iloc[:,0]:\n    sourcemaxlen = max(sourcemaxlen,len(word))","metadata":{"_uuid":"8a65a151-ca8c-4bea-95f0-52d3b3a196d3","_cell_guid":"361568ed-6329-4b68-bbff-1e9cbb6554a1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nstart of the word and end of the word are declared\n'''\nSOW_token = 0\nEOW_token = 1\n\n'''\nLang class is created and two instances of it are created which are for source and target language\n'''\nclass Lang:\n    def __init__(self):\n        self.char2index = {'-' : '-'}\n        self.index2char = {0: '0', 1: '1'} #'0' sow and '1' eow\n        self.n_chars = 2  # Count SOS and EOS\n\n\n    def addWord(self, word):\n        for c in word:\n            if c not in self.char2index:\n                self.char2index[c] = self.n_chars\n                self.index2char[self.n_chars] = c\n                self.n_chars += 1","metadata":{"_uuid":"a22ec7d0-94bb-4986-8bad-72b7e23645c9","_cell_guid":"72ef9b60-960e-46d6-a11b-cf14dbc9897e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang_input = Lang()\nlang_output = Lang()\n# li = list(df_train['X'])","metadata":{"_uuid":"ce35579e-b7e3-438d-b5a9-e7478133a6f4","_cell_guid":"1b747b74-a71c-4d5f-8a8d-ca9322701e73","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAll the unique character present in both source and target words are stored and given unique number \nTrain Test Valid set all are used to know the unique characters of the words \nOtherwise good mapping of characters to number would not be possible\n'''\n\nenglish_words_train = list(df_train['X'])\nfor word in english_words_train:\n    lang_input.addWord(word)\nhindi_words_train = list(df_train['y'])\nfor word in hindi_words_train:\n    lang_output.addWord(word)\n    \n    \nenglish_words_valid = list(df_valid['X'])\nfor word in english_words_valid:\n    lang_input.addWord(word)\nhindi_words_valid = list(df_valid['y'])\nfor word in hindi_words_valid:\n    lang_output.addWord(word)\n    \n    \nenglish_words_test = list(df_test['X'])\nfor word in english_words_test:\n    lang_input.addWord(word)\nhindi_words_test = list(df_test['y'])\nfor word in hindi_words_test:\n    lang_output.addWord(word)","metadata":{"_uuid":"ff9fd937-1232-41f9-b6e3-fc951314f189","_cell_guid":"09fb2a24-d8c8-46c0-b435-71b439bf50f7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def indexesFromWord(lang, word):\n    li = []\n    for c in word:\n        li.append(lang.char2index[c])\n    return li\n\n\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)","metadata":{"_uuid":"296e363d-650b-41da-9b3d-8d87f928e546","_cell_guid":"10ac1830-6cdf-40f2-933e-fc32bfda60b1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imax = 30\nfor i in range(len(english_words_train)):\n    for j in range(imax - len(english_words_train[i])):\n        english_words_train[i] = english_words_train[i] + '1'","metadata":{"_uuid":"a57d09bd-e6da-4115-98d7-dbd927107754","_cell_guid":"17492343-9a9a-4f35-bf08-7256d1adca6d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nEncoder class is here .\nIt extends nn of pytorch \ninput_size = Vocabulary size of source language(integer)\nhidden_size = number of neurons in the hidden layer of the encoder(integer)\ndrop_out = probability of a node being dropped out(number between 0 to 1)\nnum_layers = How many layers does the encoder has , These layers are stacked, one above another(Integer)\nbatchsize = total number of word pair in a batch(Integer)\nembeddingsize = The input (source word) characters are each converted to some embedding \n                by passing through one embedding layer , this denotes the size of that \n                embedding layer(Integer)\nbidirectional = if the encoder is bidirectional or not (takes boolean True / False)\nmodelname = name of the model (can be RNN , GRU , LSTM )(str) \ninput = Batch of words or a single word\nencoder_hidden = a tuple containing ( hidden , cell ) , LSTM has cell , so I kept a cell \n                It is used when needed , hidden and cell are initialised with initHidden method\nreturns output , a tuple containing hidden and cell state\n'''\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size,drop_out,num_layers,batchsize,embeddingsize,bidirectional,modelname):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.batchsize = batchsize\n        self.drop_out = drop_out\n        self.num_layers = num_layers\n        self.embeddingsize = embeddingsize\n        self.embedding = nn.Embedding(input_size, embeddingsize)\n        self.bidirectional = bidirectional\n        self.D = 1\n        if self.bidirectional == True:\n            self.D = 2\n        self.modelname = modelname\n        if modelname == 'RNN':\n            self.model = nn.RNN(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n        elif modelname == 'GRU':\n            self.model = nn.GRU(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n        elif modelname == 'LSTM':\n            self.model = nn.LSTM(input_size = embeddingsize, hidden_size = hidden_size,num_layers = num_layers,bidirectional = self.bidirectional)\n\n        self.dropout = nn.Dropout(self.drop_out)\n\n    def forward(self, input, encoder_hidden):\n        embedded = self.embedding(input).view(-1,self.batchsize,self.embeddingsize)\n        maxlength = embedded.size()[0]\n        (hidden ,cell) = encoder_hidden\n        output = self.dropout(embedded)\n\n    \n        if self.modelname == 'RNN':\n            output, hidden = self.model(output, hidden)\n            cell = None\n        elif self.modelname == 'GRU':\n            output, hidden = self.model(output, hidden)\n            cell = None\n        elif self.modelname == 'LSTM':\n            output, (hidden,cell) = self.model(output, (hidden,cell))\n\n        if self.D == 2:\n            h = 0\n            o = 0\n            newoutput = torch.zeros(maxlength, self.batchsize, self.hidden_size, device = device)\n            newhidden = torch.zeros(self.num_layers, self.batchsize, self.hidden_size, device = device)\n            for i in range(0,self.D * self.num_layers,2):\n                newhidden[h, : , :] = torch.div(hidden[i, :, :].add(hidden[i + 1, :, :]),2)\n                h += 1\n            hidden = newhidden\n            for i in range(0,self.D * self.hidden_size,2):\n                newoutput[: , : ,o] = torch.div(output[: ,: ,i].add(output[: , : ,i + 1]),2)\n                o += 1\n            output = newoutput\n            if cell != None:\n                c = 0\n                newcell = torch.zeros(self.num_layers, self.batchsize, self.hidden_size, device = device)\n                for i in range(0,self.D * self.num_layers,2):\n                    newcell[c, : , :] = torch.div(cell[i, :, :].add(cell[i + 1, :, :]),2)\n                    c += 1\n                cell = newcell\n        return output, (hidden,cell)\n\n    def initHidden(self):\n        return torch.zeros(self.D * self.num_layers, self.batchsize, self.hidden_size, device=device)","metadata":{"_uuid":"56e7a4ac-1f7d-4090-acc6-a1aefb437590","_cell_guid":"c6d3095c-5186-4dc0-ac5d-ee98c9bf44ad","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nDecoder class is here .\nIt extends nn of pytorch \nhidden_size = number of neurons in the hidden layer of the encoder(integer)\noutput_size = Vocabulary size of target language(integer)\ndrop_out = probability of a node being dropped out(number between 0 to 1)\nnum_layers = How many layers does the decoder has , These layers are stacked, one above another(Integer)\nbatchsize = total number of word pair in a batch(Integer)\nembeddingsize = The word (target word) characters are each converted to some embedding \n                by passing through one embedding layer , this denotes the size of that \n                embedding layer(Integer)\nmodelname = name of the model (can be RNN , GRU , LSTM )(str) \ninput = Batch of words or a single word or batch of words at a timestamp \nencoder_hidden = a tuple containing ( hidden , cell ) , LSTM has cell , so I kept a cell \n                It is used when needed , hidden and cell are initialised with initHidden method\nencoder_output = output of encoder (needed in attention implementation)\n\n\nreturns output , a tuple of hidden and cell state , attention weights (None as it is a normal decoder)\n'''\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size,drop_out,num_layers,batchsize,embeddingsize,modelname):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.batchsize = batchsize\n        self.num_layers = num_layers\n        self.drop_out = drop_out\n        self.embeddingsize = embeddingsize\n        self.embedding = nn.Embedding(output_size, embeddingsize)\n        self.modelname = modelname\n        if modelname == 'RNN':\n            self.model = nn.RNN(embeddingsize, hidden_size,num_layers)\n        elif modelname == 'GRU':\n            self.model = nn.GRU(embeddingsize, hidden_size,num_layers)\n        elif modelname == 'LSTM':\n            self.model = nn.LSTM(embeddingsize, hidden_size,num_layers)\n\n        self.dropout = nn.Dropout(self.drop_out)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, encoder_hidden,encoder_output):\n        output = self.embedding(input).view(-1, self.batchsize, self.embeddingsize)\n        output = self.dropout(output)\n        output = F.relu(output)\n        (hidden, cell) = encoder_hidden\n        if self.modelname == 'RNN':\n            output, hidden = self.model(output, hidden)\n        elif self.modelname == 'GRU':\n            output, hidden = self.model(output, hidden)\n        elif self.modelname == 'LSTM':\n            output, (hidden,cell) = self.model(output, (hidden,cell))\n        \n        output = self.softmax(self.out(output[0]))#dim = [128,67] before applying softmax\n        return output, (hidden,cell),None","metadata":{"_uuid":"c8076931-d1f2-4970-a0fd-8e3833fdd4b4","_cell_guid":"43005a81-408d-4928-95f7-397b2c4c8ab5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAttnDecoder class is here .\nIt extends nn of pytorch \nhidden_size = number of neurons in the hidden layer of the encoder(integer)\noutput_size = Vocabulary size of target language(integer)\ndrop_out = probability of a node being dropped out(number between 0 to 1)\nnum_layers = How many layers does the decoder has , These layers are stacked, one above another(Integer)\nbatchsize = total number of word pair in a batch(Integer)\nembeddingsize = The word (target word) characters are each converted to some embedding \n                by passing through one embedding layer , this denotes the size of that \n                embedding layer(Integer)\nmodelname = name of the model (can be RNN , GRU , LSTM )(str) \ninput = Batch of words or a single word or batch of words at a timestamp \nencoder_hidden = a tuple containing ( hidden , cell ) , LSTM has cell , so I kept a cell \n                It is used when needed , hidden and cell are initialised with initHidden method\nencoder_output = output of encoder (needed in attention implementation)\n\n\nreturns output , a tuple of hidden and cell state , attention weights (None as it is a normal decoder)\n'''\n\nclass AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size,drop_out,num_layers,batchsize,embeddingsize,modelname):\n        super(AttnDecoderRNN, self).__init__()\n        maxlength = imax + 1\n        self.hidden_size = hidden_size\n        self.batchsize = batchsize\n        self.num_layers = num_layers\n        self.drop_out = drop_out\n        self.embeddingsize = embeddingsize\n        self.embedding = nn.Embedding(output_size, embeddingsize)\n        self.modelname = modelname\n        if modelname == 'RNN':\n            self.model = nn.RNN(embeddingsize, hidden_size,num_layers)\n        elif modelname == 'GRU':\n            self.model = nn.GRU(embeddingsize, hidden_size,num_layers)\n        elif modelname == 'LSTM':\n            self.model = nn.LSTM(embeddingsize, hidden_size,num_layers)\n#         self.gru = nn.GRU(embeddingsize, hidden_size,num_layers,dropout = drop_out,bidirectional = False)\n        self.dropout = nn.Dropout(self.drop_out)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n        self.attn = nn.Linear(self.hidden_size + self.embeddingsize, maxlength).to(device)\n        self.attn_combine = nn.Linear(self.hidden_size + self.embeddingsize, self.embeddingsize).to(device)\n    def forward(self, input, encoder_hidden,encoder_output):\n        \n        (hidden, cell) = encoder_hidden\n        output = self.embedding(input).view(-1, self.batchsize, self.embeddingsize)\n        output = self.dropout(output)\n        cat = torch.cat((output[0], hidden[0]), 1)\n        attn_weights = F.softmax(\n            self.attn(cat), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n                                 torch.permute(encoder_output,(1,0,2)))\n\n        output = torch.cat((output[0], attn_applied.squeeze(1)), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        \n        output = F.relu(output)\n        if self.modelname == 'RNN':\n            output, hidden = self.model(output, hidden)\n        elif self.modelname == 'GRU':\n            output, hidden = self.model(output, hidden)\n        elif self.modelname == 'LSTM':\n            output, (hidden,cell) = self.model(output, (hidden,cell))\n        \n        output = self.softmax(self.out(output[0])) #dim = [128,67] before applying softmax\n        return output, (hidden,cell),attn_weights","metadata":{"_uuid":"18692bb7-bcfd-42ab-ad83-61f4ad5492a9","_cell_guid":"b133fec7-884e-4b15-9465-102c4f5caf2f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis is train function \nAt first a teacher forcing ratio is given , it is 0.5 here\ninput_tensor = one batch of tensor of source words . They are preprocessed .\n                so , every tensor in a batch is of same size as ,  every word ends by EOW token\n                and then they are padded to reach the maximum length of source word in that batch\n\ntarget_tensor = True target tensor words corresponding to the batch of input tensor . Like \n                input tensor these are also preprocessed , They end with EOW token and then\n                they are padded to reach the maximum length of words in that batch\nencoder = encoder model instance\ndecoder = decoder model instance\nencoder_optimizer = The optimizer used for the encoder ( Can be SGD , RMSprop , Adam , NAdam)\ndecoder_optimizer = The optimizer used for the decoder ( Can be SGD , RMSprop , Adam , NAdam)\ncriterion = It means The nn module's CrossEntropyLoss instance\nbatchsize = batchsize of the model (valid for both encoder and decoder)\nIt returns the loss in that batch and also the word level accuracy in that batch\n'''\n\nteacher_forcing_ratio = 0.5\n\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batchsize):\n    hidden = encoder.initHidden()\n    cell = hidden\n    encoder_hidden = (hidden, cell)\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n    # encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n    accu = 0\n    # for ei in range(input_length):\n    #     encoder_output, encoder_hidden = encoder(\n    #         input_tensor[ei], encoder_hidden)\n    #     encoder_outputs[ei] = encoder_output[0, 0]\n    # print(input_tensor.shape)\n#     print(input_tensor)\n    encoder_output, encoder_hidden = encoder(\n            input_tensor, encoder_hidden)\n    decoder_input = torch.tensor([SOW_token] * batchsize, device=device)\n    d_hidden = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n    d_cell = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n    (hidden, cell) = encoder_hidden \n    if encoder.num_layers != decoder.num_layers:\n        d_hidden[:,:,:] = hidden[encoder.num_layers - 1,:,:]\n        if cell != None:\n            d_cell[:,:,:] = cell[encoder.num_layers - 1,:,:]\n        else :\n            d_cell = cell\n    else :\n        d_hidden = hidden\n        d_cell = cell\n    if d_cell == None and decoder.modelname == 'LSTM':\n        d_cell = d_hidden\n    decoder_hidden = (d_hidden , d_cell)\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n    decoder_words = []\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden ,attn_weights= decoder(\n                decoder_input, decoder_hidden,encoder_output)\n            \n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n            topv, topi = decoder_output.topk(1)\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n#             decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n      \n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden,attn_weights = decoder(\n                decoder_input, decoder_hidden ,encoder_output)\n            loss += criterion(decoder_output, target_tensor[di])\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n#             decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n            \n       \n    loss.backward()\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    accu = accuracy(decoder_words,target_tensor)\n    return loss.item() / target_length,accu","metadata":{"_uuid":"3b630d26-fd17-429c-bc33-97f4daf635ba","_cell_guid":"d2249a24-d02c-4cee-90a2-59496f27fcc1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nencoder = encoder model instance\ndecoder = decoder model instance\nn_iters = total number of iteration of training on the whole training set or, total number of epochs\nlearning_rate = learning_rate of the optimizer model\nbatchsize = batchsize of the model\noptimizername = name of the optimizer  model used \nbeta = used in SGD for momentum hyper parameter\n\nThis function takes both source and target words , process them (changes the characters to integers) and pads \niff necessery to make each word of same length in a batch \nThen it runs total n_iters time \nIn each of them it trains each of the batch of training set and prints the training loss \n, validation loss , training accuracy , validation accuracy(after completion of an epoch)\n\n'''\ndef trainIters(encoder, decoder, n_iters, learning_rate,batchsize,optimizername,beta):\n    loss_total = 0  # Reset every print_every\n    accu_total = 0\n    if optimizername == 'Adam':\n        encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'NAdam':\n        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'RMSprop':\n        encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=learning_rate)\n        decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=learning_rate)\n    elif optimizername == 'SGD':\n        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate,momentum = beta)\n        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate,momentum = beta)\n    criterion = nn.CrossEntropyLoss()\n    \n    input_batch = []\n    output_batch = []\n    input_tensor = [tensorFromWord(lang_input,word) for word in english_words_train]\n    output_tensor = [tensorFromWord(lang_output,word) for word in hindi_words_train]\n\n    \n    \n    for i in range(0,len(english_words_train),batchsize):\n        if i + batchsize > len(english_words_train):\n            break\n        input_batch.append(nn.utils.rnn.pad_sequence(input_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n        output_batch.append(nn.utils.rnn.pad_sequence(output_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batch\n    \n\n\n    for iter in range(1, n_iters + 1):\n        for batchnum in range(len(input_batch)):\n#             print(input_batch[batchnum])\n            loss,accu = train(input_batch[batchnum], output_batch[batchnum], encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batchsize)\n            if batchnum % 500 == 0:\n                print(loss,batchnum)\n                pass\n            loss_total += loss * batchsize\n            accu_total = accu_total + accu\n        training_loss = loss_total/len(english_words_train)\n        training_accuracy = (accu_total * 100)/(len(input_batch) * batchsize)\n        validation_loss , validation_accuracy = evaluate(encoder,decoder,df_valid,batchsize)\n        print(\"training loss after epoch no {}\".format(iter) ,training_loss)\n        print(\"training accuracy after epoch no {}\".format(iter),training_accuracy)\n        print(\"validation loss after epoch no {}\".format(iter),validation_loss)\n        print(\"validation accuracy after epoch no {}\".format(iter),validation_accuracy)\n#         wandb.log({\"training_accuracy\": training_accuracy, \"validation_accuracy\": validation_accuracy, \"training_loss\": training_loss, \"validation_loss\": validation_loss,\"Epoch\" : iter})\n\n        loss_total = 0\n        accu_total = 0","metadata":{"_uuid":"7332551b-bcb4-4765-adee-08f65286b804","_cell_guid":"6e8aa605-bc2d-4e70-897e-ea39180ec6c0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nIt is the method which given predictedlist and actuallist returns accuracy\npredictedlist = predicted word list(target word)\nactuallist = actual word list(target word)\nIt calculates word level accuracy\n'''\n\ndef accuracy(predictedlist, actuallist):\n    accu = 0\n    N = actuallist.size()[1]\n    L = actuallist.size()[0]\n    predictedtensor = torch.zeros(L,N).to(device)\n    for i in range(L):\n        predictedtensor[i] = predictedlist[i]\n#     print(predictedtensor)\n    for i in range(N):\n        if torch.equal(predictedtensor[:,i],actuallist[:,i]) == True:\n            accu += 1\n        \n    return accu","metadata":{"_uuid":"1b939602-67e1-4263-9592-478f729c6eed","_cell_guid":"5b8d99a1-2a31-4d22-8423-7490826eacc5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nencoder = encoder model instance\ndecoder = decoder model instance\ndataset = dataset on which to evaluate the model\nbatchsize = number of words in a batch\ncalculates loss and accuracy of the model on the given dataset\n'''\n\ndef evaluate(encoder,decoder,dataset,batchsize):\n    loss_total = 0\n    accu_total = 0\n    input_batch = []\n    target_batch = []\n    encoder.eval()\n    decoder.eval()\n    criterion = nn.CrossEntropyLoss()\n    result = []\n    english_words = list(dataset['X'])\n    hindi_words = list(dataset['y'])\n    imax = 30\n    for i in range(len(english_words_valid)):\n        for j in range(imax - len(english_words_valid[i])):\n            english_words_valid[i] = english_words_valid[i] + '1'\n\n    input_tensor = [tensorFromWord(lang_input,word) for word in english_words_valid]\n    output_tensor = [tensorFromWord(lang_output,word) for word in hindi_words_valid]\n\n    \n    \n    for i in range(0,len(english_words),batchsize):\n        if i + batchsize > len(english_words):\n            break\n        input_batch.append(nn.utils.rnn.pad_sequence(input_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batchsize,1\n        target_batch.append(nn.utils.rnn.pad_sequence(output_tensor[i:i + batchsize]).squeeze(2).to(device)) #maxlength,batch\n    \n    \n    loss = 0\n    for batchnum in range(len(input_batch)):\n        hidden = encoder.initHidden()\n        cell = hidden\n        encoder_hidden = (hidden, cell)\n    \n        input_length = input_batch[batchnum].size(0)\n        target_length = target_batch[batchnum].size(0)\n    \n        encoder_output, encoder_hidden = encoder(\n            input_batch[batchnum], encoder_hidden)\n        decoder_input = torch.tensor([SOW_token] * batchsize, device=device)\n\n        \n        \n        d_hidden = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n        d_cell = torch.zeros(decoder.num_layers, decoder.batchsize, decoder.hidden_size, device = device)\n        (hidden, cell) = encoder_hidden \n        if encoder.num_layers != decoder.num_layers:\n            d_hidden[:,:,:] = hidden[encoder.num_layers - 1,:,:]\n            if cell != None:\n                d_cell[:,:,:] = cell[encoder.num_layers - 1,:,:]\n            else :\n                d_cell = cell\n        else :\n            d_hidden = hidden\n            d_cell = cell\n        if d_cell == None and decoder.modelname == 'LSTM':\n            d_cell = d_hidden\n        decoder_hidden = (d_hidden , d_cell)\n\n        decoder_words = []\n        res_words = []\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden,attn_weights = decoder(\n                decoder_input, decoder_hidden,encoder_output)\n            loss += criterion(decoder_output, target_batch[batchnum][di])\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n            decoder_words.append(topi.squeeze().detach().view(1,-1))\n            res_words.append(topi.squeeze().detach().view(-1,1))\n    #       decoder_words.append([lang_output.index2char[decoder_output[i]] for i in decoder_output.size()[0])\n        accu = accuracy(decoder_words,target_batch[batchnum])\n    \n        loss = loss.item() / target_length\n        loss_total += loss * batchsize\n        accu_total += accu\n\n        for i in range(batchsize):\n            resultantlist = []\n            for j in range(target_length):\n                if res_words[j][i].item() == 1:\n                    break\n                resultantlist.append(lang_output.index2char[res_words[j][i].item()])\n            result.append(resultantlist)\n    loss_total = loss_total/len(english_words)\n    accu_total = (accu_total * 100)/(len(input_batch) * batchsize)\n    print(\"total loss in validation set = \", loss_total)\n    print('total accuracy in validation set = ',accu_total)\n#     finalresult = []\n#     for i in range(len(result)):\n#         finalresult.append(''.join(result[i]))\n#     print(dataset['y'][1])\n#     print(len(finalresult))\n#     correct = 0\n#     for i in range(len(finalresult)):\n#         if finalresult[i] == dataset['y'][i]:\n#             correct += 1\n#     print(correct)\n    \n#     with open('output.txt' , 'w') as fp:\n#         fp.write(\"correct output    \")\n#         fp.write(\"predicted output\\n\")\n#         fp.write(\"--------------------------------------------\\n\")\n#         for i in range(len(finalresult)):\n#             fp.write(\"--------------------------------------------\\n\")\n#             fp.write(f\"{dataset['y'][i]:<40}{finalresult[i]:<50}\")\n#             fp.write('\\n')\n#             fp.write(\"--------------------------------------------\\n\")\n# #             fp.write(\"%s\\n\"%finalresult[i])\n#     fp.close()\n    \n    \n#     vowelerror = 0\n#     consonanterror = 0\n#     vowelcorrect = 0\n#     consonantcorrect = 0\n#     for i in range(len(result)):\n#         truelist = list(dataset['y'][i])\n#         for j in range(min(len(truelist) , len(result[i]))):\n#             if is_vowel(truelist[j] , lang):\n#                 if  truelist[j] != result[i][j]:\n#                     vowelerror += 1\n#                 else:\n#                     vowelcorrect += 1\n#             elif is_consonant(truelist[j] , lang):\n#                 if  truelist[j] != result[i][j]:\n#                     consonanterror += 1\n#                 else:\n#                     consonantcorrect += 1\n#         if len(truelist) > len(result[i]):\n#             for j in range(len(result[i]) , len(truelist)):\n#                 if is_vowel(truelist[j] ,lang):\n#                     vowelerror += 1\n#                 else:\n#                     consonanterror += 1\n    \n    \n#     print(vowelerror)\n#     print(vowelcorrect)\n#     print(consonanterror)\n#     print(consonantcorrect)\n    encoder.train()\n    decoder.train()\n    return loss_total,accu_total","metadata":{"_uuid":"de3be510-42ea-43cf-a66c-96326352ceeb","_cell_guid":"c0e46fb7-f1ed-40f0-ab51-d0086dd7e44b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best configuration\nembedding_size = 512\nnum_encoder_layers = 3\nnum_decoder_layers = 1\nhidden_size = 1024\nlearning_rate = 0.001\nbeta = 0\noptimizer = 'Adam'\nbatchsize = 128\nbidirectional = True\nn_iters = 30\nloss = 'cross_entropy'\nencodermodelname = 'LSTM'\ndecodermodelname = 'LSTM'\ndrop_out = 0.2\nattn = False","metadata":{"_uuid":"e8a76aa2-6915-4430-952d-5d3d36cc369e","_cell_guid":"678d195d-e33c-463f-bb2e-f0e792c7a666","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if args.embeddingsize is not None:\n    embedding_size = args.embeddingsize\nif args.number_of_encoder_layers is not None:\n    num_encoder_layers = args.number_of_encoder_layers\nif args.number_of_decoder_layers is not None:\n    num_decoder_layers = args.number_of_decoder_layers\nif args.hiddensize is not None:\n    hidden_size = args.hiddensize\nif args.learning_rate is not None:\n    learning_rate = args.learning_rate\nif args.beta is not None:\n    beta = args.beta\nif args.optimizer is not None:\n    optimizer = args.optimizer\nif args.batch_size is not None:\n    batchsize = args.batch_size\nif args.bidirectional is not None:\n    bidirectional = args.bidirectional\nif args.num_iteration is not None:\n    n_iters = args.num_iteration\nif args.loss is not None:\n    loss = args.loss\nif args.encodermodelname is not None:\n    encodermodelname = args.encodermodelname\nif args.decodermodelname is not None:\n    decodermodelname = args.decodermodelname\nif args.drop_out is not None:\n    drop_out = args.drop_out\nif args.attention is not None:\n    attn = args.attention","metadata":{"_uuid":"74258869-c41e-4de2-9c5b-e8030b624838","_cell_guid":"096fea65-b913-4e1a-9c30-2e932a363630","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    '''\n    default of project name and entity name'''\n    project_name = 'Assignment 2'\n    entity_name = 'cs22m084'\n    \n    '''giving the user given value if can be given'''\n    if args.wandb_project is not None:\n        project_name = args.wandb_project\n    if args.wandb_entity is not None:\n        entity_name = args.wandb_entity\n    \n    '''initialize sweep '''\n    '''building NN model and training the data on this model'''\n    wandb.init(project = project_name,entity = entity_name)\n    \n    run_name = \"es_{}_nenl_{}_ndel_{}_hs_{}_lr_{}_bt_{}_o_{}_bs_{}_bi_{}_n_iters_{}_loss_{}_emn_{}_dmn_{}_do_{}_attn_{}\".format(embedding_size,num_encoder_layers,num_decoder_layers,hidden_size,learning_rate,beta,optimizer,batchsize,bidirectional,n_iters,loss,encodermodelname,decodermodelname,drop_out,attn)\n    print(\"run name - \", run_name)\n    wandb.run.name = run_name\n    encoder = EncoderRNN(lang_input.n_chars,hidden_size ,drop_out,num_encoder_layers,batchsize,embedding_size,bidirectional,encodermodelname).to(device)\n    decoder1 = DecoderRNN(hidden_size, lang_output.n_chars,drop_out,num_decoder_layers,batchsize,embedding_size,decodermodelname).to(device)\n    decoder2 = AttnDecoder(hidden_size, lang_output.n_chars,drop_out,num_decoder_layers,batchsize,embedding_size,decodermodelname).to(device)\n    \n    \n    if attn == True:\n        decoder = decoder2\n    else:\n        decoder = decoder1\n        \n    trainIters(encoder,decoder,n_iters,learning_rate,batchsize,optimizer,beta)","metadata":{"_uuid":"c3a4a1bd-921d-4b4c-8eb3-1a37552d5d04","_cell_guid":"d4cf2694-3a1d-477b-ae30-8a1d047892aa","collapsed":false,"execution":{"iopub.status.busy":"2023-05-21T15:45:11.778690Z","iopub.execute_input":"2023-05-21T15:45:11.779081Z","iopub.status.idle":"2023-05-21T15:45:11.788387Z","shell.execute_reply.started":"2023-05-21T15:45:11.779051Z","shell.execute_reply":"2023-05-21T15:45:11.787276Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"43592bce-b605-46a4-930f-0649e5193267","_cell_guid":"1e575e68-800c-4150-9ba9-273f7c29fec7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"682362f1-6508-4b7d-b1e3-e658f87362f0","_cell_guid":"3159dfcb-4a16-482c-aa88-66f438c0ab52","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}